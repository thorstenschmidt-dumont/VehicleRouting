{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXu1r8qvSzWf"
   },
   "source": [
    "# REINFORCE actor-critic learning for the CVRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xjm2onHdT-Av"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ikr2p0Js8iB4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from gym import wrappers\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2mkXSZ-JWOr"
   },
   "source": [
    "## Creating a CVRP environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5A98HsVVJbT5"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "\n",
    "class VRPEnv(gym.Env):\n",
    "  def __init__(self):\n",
    "    # customer count ('0' is depot) \n",
    "    self.customer_count = 11\n",
    "    # the capacity of vehicles\n",
    "    self.vehicle_capacity = 2\n",
    "  \n",
    "    self.action_space = spaces.Discrete(3)\n",
    "    self.observation_space = spaces.Box(low=0,high=1, shape=(4,1), dtype=np.float64)\n",
    "    self.VRP = np.array((self.customer_count,4))\n",
    "    self._max_episode_steps = 1000\n",
    "    self.viewer = None\n",
    "    self.state = None\n",
    "    self.steps_beyond_done = None\n",
    "    self.route = []\n",
    "    self.route.append(0)\n",
    "    self.previous_action = 0\n",
    "    self.hn_actor = torch.zeros([1,self.customer_count,128], dtype=torch.float32).to(device)\n",
    "    self.hn_actor_target = torch.zeros([1,self.customer_count,128], dtype=torch.float32).to(device)\n",
    "    \n",
    "\n",
    "  def reset(self, seed=200):\n",
    "    if seed == 200:\n",
    "      seed = int(time.time())\n",
    "    np.random.seed(seed)\n",
    "    x_locations = (np.random.rand(self.customer_count)).reshape((self.customer_count,1))\n",
    "    y_locations = (np.random.rand(self.customer_count)).reshape((self.customer_count,1))\n",
    "    demand = (np.random.randint(1,9,self.customer_count).reshape((self.customer_count,1))).reshape((self.customer_count,1))/10 # Normalise to between 0.1 and 0.9\n",
    "    capacity = np.repeat(self.vehicle_capacity,self.customer_count).reshape((self.customer_count,1))\n",
    "    VRP = np.concatenate((np.concatenate((np.concatenate((x_locations,y_locations), axis=1),demand),axis=1),capacity),axis=1)\n",
    "    self.VRP = VRP.reshape((self.customer_count,4))\n",
    "    self.unserved_customers = []\n",
    "    for i in range(1, self.customer_count):\n",
    "      self.unserved_customers.append(i)\n",
    "    self.routes = []\n",
    "    self.route = []\n",
    "    self.route.append(0)\n",
    "    self.VRP[0,2] = 0 # Set the demand at thedepot to 0\n",
    "    self.state = copy.deepcopy(self.VRP)\n",
    "    self.previous_action = 0\n",
    "    return self.state\n",
    "  \n",
    "\n",
    "  def step(self, action):\n",
    "    # Calculate the reward as the negative euclidean distance\n",
    "    reward = -((self.state[self.previous_action,0]-self.state[action,0])**2+(self.state[self.previous_action,1]-self.state[action,1])**2)**0.5 # - Euclidean distance between customers\n",
    "    load = self.state[0,3]\n",
    "    self.state[:,3] = max(0,(load-self.state[action,2])) # Update the vehicle load\n",
    "    self.state[action, 2] = max(0,self.state[action,2]-load) # Update the demand at served customer\n",
    "    done = False\n",
    "    if action == 0: # Return to the depot\n",
    "      self.route.append(action) # End route\n",
    "      self.routes.append(self.route) # Add subroute to list of all routes\n",
    "      self.route = [] # Initiate new subroute\n",
    "      self.state[:,3] = self.vehicle_capacity # Refill the vehicle\n",
    "    self.route.append(action) # Add action to the subroute\n",
    "    if max(self.state[:,2]) > 0: # If there are unserved customers left\n",
    "      done = False\n",
    "    elif max(self.state[:,2]) == 0 and action == 0: # If there are no unserved customers left and we have returned to the depot\n",
    "      done = True\n",
    "      self.route.append(0)\n",
    "    self.previous_action = action # Update the previous action\n",
    "    return self.state, reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyy2xkZRNCRw"
   },
   "source": [
    "## Let's test the environment step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_xu5wGbNINA",
    "outputId": "7055ef74-5482-4701-a5f7-90a14d846136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45621329 0.66205604 0.         2.        ]\n",
      " [0.2814365  0.16862356 0.5        2.        ]\n",
      " [0.0031236  0.04988002 0.2        2.        ]\n",
      " [0.48095493 0.37237519 0.2        2.        ]\n",
      " [0.27976012 0.9823364  0.1        2.        ]\n",
      " [0.2603242  0.33208958 0.1        2.        ]\n",
      " [0.99899722 0.13060665 0.4        2.        ]\n",
      " [0.49919138 0.14070669 0.8        2.        ]\n",
      " [0.23995861 0.32464986 0.7        2.        ]\n",
      " [0.64083404 0.37304389 0.1        2.        ]\n",
      " [0.42405637 0.39863593 0.2        2.        ]]\n",
      "[[0.45621329 0.66205604 0.         1.8       ]\n",
      " [0.2814365  0.16862356 0.5        1.8       ]\n",
      " [0.0031236  0.04988002 0.         1.8       ]\n",
      " [0.48095493 0.37237519 0.2        1.8       ]\n",
      " [0.27976012 0.9823364  0.1        1.8       ]\n",
      " [0.2603242  0.33208958 0.1        1.8       ]\n",
      " [0.99899722 0.13060665 0.4        1.8       ]\n",
      " [0.49919138 0.14070669 0.8        1.8       ]\n",
      " [0.23995861 0.32464986 0.7        1.8       ]\n",
      " [0.64083404 0.37304389 0.1        1.8       ]\n",
      " [0.42405637 0.39863593 0.2        1.8       ]]\n"
     ]
    }
   ],
   "source": [
    "env = VRPEnv() # Create an instance of the environment\n",
    "state = env.reset() # Reset the environment\n",
    "action = 2 # Perform action with customer 2\n",
    "print(state)\n",
    "state, reward, done = env.step(action) # Perform the actual transition\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2nGdtlKVydr"
   },
   "source": [
    "## Initialize the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "u5rW0IDB8nTO"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, max_size=1e6):\n",
    "    self.storage = []\n",
    "    self.max_size = max_size\n",
    "    self.ptr = 0\n",
    "\n",
    "  def add(self, transition):\n",
    "    if len(self.storage) == self.max_size:\n",
    "      #self.storage[int(self.ptr)] = transition\n",
    "      #self.ptr = (self.ptr + 1) % self.max_size\n",
    "      self.storage.pop(0)\n",
    "      self.storage.append(transition)\n",
    "    else:\n",
    "      self.storage.append(transition)\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    ind =  np.arange((len(self.storage)-(batch_size+1)),len(self.storage)-1,1) #np.random.randint(0, len(self.storage), size=batch_size)\n",
    "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "    for i in ind: \n",
    "      state, next_state, action, reward, done = self.storage[i]\n",
    "      batch_states.append(np.array(state, copy=False))\n",
    "      batch_next_states.append(np.array(next_state, copy=False))\n",
    "      batch_actions.append(np.array(action, copy=False))\n",
    "      batch_rewards.append(np.array(reward, copy=False))\n",
    "      batch_dones.append(np.array(done, copy=False))\n",
    "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb7TTaHxWbQD"
   },
   "source": [
    "## Build the neural network for the Actor and Actor-target models - contains the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4CeRW4D79HL0"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim=4, embed_size = 128):#, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.embed = nn.Linear((state_dim), embed_size) # Encoding to higher dimensional space - can also be changed to convolutional layer as in the paper\n",
    "    self.u_t = nn.RNN(embed_size,embed_size,1) # RNN Layer for the attention mechanism\n",
    "    self.v_t_a = nn.Linear(embed_size,1) # Linear for getting u_t\n",
    "    self.bar_u_t = nn.RNN(embed_size,embed_size,1) # RNN Layer for the context vector\n",
    "    self.a_t = nn.Softmax(dim = 1) # Softmax layer for the attention mechanism\n",
    "    self.v_t_u = nn.Linear(embed_size,1) # Linear for getting u_t\n",
    "    self.final = nn.Softmax(dim = 1) # Softmax layer for the final output\n",
    "\n",
    "    self.saved_log_probabilities = [] # Create and empty list for saving the log probabilities of the actions\n",
    "    self.rewards = [] # Create an empty list for the rewards\n",
    "    self.dones = [] # Create an empty list for checking whether the episode was completed\n",
    "\n",
    "  def forward(self, x, hn = env.hn_actor):\n",
    "    cond1 = (x[:,:,2]<x[:,:,3]).int() # Can we meet the demand\n",
    "    cond2 = (x[:,:,2]>0).int() # Is there demand at the customer\n",
    "    mask1 = torch.minimum(cond1,cond2) # Select those customers with demand, and whose demand we can meet\n",
    "    if torch.sum(mask1[:,1:len(mask1[0])]) == 0: # If only the depot can be visited\n",
    "      mask1[:,0] = 1\n",
    "    mask1 = torch.reshape(mask1,(len(x),env.customer_count,1))\n",
    "    x = self.embed(x)\n",
    "    u, hn = self.u_t(x, hn)\n",
    "    u = self.v_t_a(u)\n",
    "    a = self.a_t(u) # Up to equation (4) now\n",
    "    c = torch.randn(x.shape)\n",
    "    c = torch.mul(x,a)\n",
    "    c = torch.sum(c, 0)\n",
    "    c = torch.reshape(c,(1,env.customer_count,128))\n",
    "    u_bar, hu = self.bar_u_t(x,c)\n",
    "    u_bar = self.v_t_u(u_bar)\n",
    "    output = self.final(u_bar)\n",
    "    output = torch.mul(output,mask1)\n",
    "    #print(\"Before clamp \", output)\n",
    "    #output = output.log().clamp(epsilon, 1 - epsilon)\n",
    "    epsilon = 10 ** -44\n",
    "    output = output.clamp(min=1e-4)\n",
    "    #print(\"After clamp \",output)\n",
    "    output = output.log()#.clamp(epsilon, 1 - epsilon)\n",
    "    #print(\"After log \",output)\n",
    "    output = self.final(output)\n",
    "    return output\n",
    "\n",
    "class Actor_Target(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim=4, embed_size = 128):#, action_dim, max_action):\n",
    "    super(Actor_Target, self).__init__()\n",
    "    self.embed = nn.Linear((state_dim), embed_size) # Encoding to higher dimensional space - can also be changed to convolutional layer as in the paper\n",
    "    self.u_t = nn.RNN(embed_size,embed_size,1) # RNN Layer for the attention mechanism\n",
    "    self.v_t_a = nn.Linear(embed_size,1) # Linear for getting u_t\n",
    "    self.bar_u_t = nn.RNN(embed_size,embed_size,1) # RNN Layer for the context vector\n",
    "    self.a_t = nn.Softmax(dim = 1) # Softmax layer for the attention mechanism\n",
    "    self.v_t_u = nn.Linear(embed_size,1) # Linear for getting u_t\n",
    "    self.final = nn.Softmax(dim = 1) # Softmax layer for the final output\n",
    "\n",
    "  def forward(self, x, hn = env.hn_actor_target):\n",
    "    cond1 = (x[:,:,2]<x[:,:,3]).int() # Can we meet the demand\n",
    "    cond2 = (x[:,:,2]>0).int() # Is there demand at the customer\n",
    "    mask1 = torch.minimum(cond1,cond2) # Select those customers with demand, and whose demand we can meet\n",
    "    if torch.sum(mask1[:,1:len(mask1[0])]) == 0: # If only the depot can be visited\n",
    "      mask1[:,0] = 1\n",
    "    mask1 = torch.reshape(mask1,(len(x),env.customer_count,1))\n",
    "    x = self.embed(x)\n",
    "    u, hn = self.u_t(x, hn)\n",
    "    u = self.v_t_a(u)\n",
    "    a = self.a_t(u) # Up to equation (4) now\n",
    "    c = torch.randn(x.shape)\n",
    "    c = torch.mul(x,a)\n",
    "    c = torch.sum(c, 0)\n",
    "    c = torch.reshape(c,(1,env.customer_count,128))\n",
    "    u_bar, hu = self.bar_u_t(x,c)\n",
    "    u_bar = self.v_t_u(u_bar)\n",
    "    output = self.final(u_bar)\n",
    "    output = torch.mul(output,mask1)\n",
    "    output = self.final(torch.log(output))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDRPWgEVNRcF"
   },
   "source": [
    "## Build the neural network for the Critic and Critic-target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_z9ifdVdNRLO"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim=4, action_dim = env.customer_count, embed_size = 128):\n",
    "    super(Critic, self).__init__()\n",
    "    # Defining the first Critic neural network\n",
    "    self.layer_1 = nn.Linear(state_dim, embed_size) # Perform the embedding\n",
    "    self.layer_2 = nn.Linear(embed_size, embed_size) # Adding the single dense layer\n",
    "    self.layer_3 = nn.Linear(embed_size, 1) # Adding the output layer\n",
    "\n",
    "    self.values = [] # Create an empty list to store the predicted values\n",
    "\n",
    "\n",
    "  def forward(self, x, u): # x is the state, u is the action\n",
    "    # Forward-Propagation on the Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(x))\n",
    "    ws = torch.mul(x1,u)\n",
    "    x2 = F.relu(self.layer_2(ws))\n",
    "    x2 = self.layer_3(x2)\n",
    "    x2 = torch.sum(x2,1)\n",
    "    return x2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VmkiLbHonzy"
   },
   "source": [
    "## Testing the actor and critic networks to confirm their output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQ_-V52dw76O",
    "outputId": "f41c1e91-a0df-426f-bc86-23f230e14e59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0001],\n",
      "         [0.1003],\n",
      "         [0.0984],\n",
      "         [0.1002],\n",
      "         [0.0963],\n",
      "         [0.0989],\n",
      "         [0.1035],\n",
      "         [0.1017],\n",
      "         [0.0998],\n",
      "         [0.1008],\n",
      "         [0.0998]]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1259]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "env = VRPEnv()\n",
    "env.reset()\n",
    "state = env.reset()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state = torch.Tensor(state.reshape(1,env.customer_count,4)).to(device)\n",
    "actor = Actor().to(device)\n",
    "actor_target = Actor_Target().to(device)\n",
    "prediction = actor(state)\n",
    "prediction_target = actor_target(state)\n",
    "print(prediction)\n",
    "critic = Critic().to(device)\n",
    "q_value = critic(state,prediction)\n",
    "print(q_value)\n",
    "\n",
    "state, reward, done = env.step(5)\n",
    "state = torch.Tensor(state.reshape(1,env.customer_count,4)).to(device)\n",
    "prediction = actor(state)\n",
    "print(torch.sum(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzIDuONodenW"
   },
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zzd0H1xukdKe"
   },
   "outputs": [],
   "source": [
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Building the Training Process into a class\n",
    "class Actor_Critic(object):\n",
    "  \n",
    "  def __init__(self, state_dim):\n",
    "    self.actor = Actor_Target(state_dim).to(device)\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = 0.0001)\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr = 0.0001)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state, state_dim):\n",
    "    #state_tensor = torch.Tensor(state.reshape(1,env.customer_count,state_dim)).to(device)\n",
    "    current_Q = actor(state)\n",
    "    Q = torch.reshape(current_Q,(1,env.customer_count))\n",
    "    m = Categorical(Q)\n",
    "    action = m.sample()\n",
    "    actor.saved_log_probabilities.append(m.log_prob(action))\n",
    "    return action.item(), current_Q\n",
    "\n",
    "  def select_target_action(self, state, state_dim):\n",
    "    #state_tensor = torch.Tensor(state.reshape(1,env.customer_count,state_dim)).to(device)\n",
    "    target_Q = actor(state)\n",
    "    Q = target_Q.detach().cpu().numpy().reshape(env.customer_count)\n",
    "    action = np.argmax(Q)\n",
    "    return action, target_Q\n",
    "\n",
    "  def train(self, iterations, batch_size=32, tau=0.005):\n",
    "    for it in range(iterations):\n",
    "      running_reward = 8\n",
    "      \n",
    "      # Ensuring that the variables have been cleared\n",
    "      del actor.rewards[:]\n",
    "      del actor.saved_log_probabilities[:]\n",
    "      del critic.values[:]\n",
    "      \n",
    "      returns = [] # Create an empty list for the returns for the batch of episodes\n",
    "      Steps = [] # Create an empty list for the number of steps required for each episode\n",
    "      steps = 0\n",
    "      for b in range(batch_size):\n",
    "        # Working with the actor and the critic\n",
    "        obs = env.reset(((it+1)*b))\n",
    "        obs = torch.Tensor(obs.reshape(1,env.customer_count,state_dim)).to(device)\n",
    "        InstantReward = 0\n",
    "        done = False\n",
    "        Steps.append(steps)\n",
    "        steps = 0\n",
    "        while not done:\n",
    "          # Complete episode playing actions as selected by the actor\n",
    "          action, current_Q = self.select_action(obs, state_dim)\n",
    "          obs, reward, done = env.step(action)\n",
    "          InstantReward += reward\n",
    "          actor.rewards.append(reward)\n",
    "          actor.dones.append(done)\n",
    "          \n",
    "          obs = torch.Tensor(obs.reshape(1,env.customer_count,state_dim)).to(device)\n",
    "          \n",
    "          # Predict the expected value of the action with the critic\n",
    "          predicted_value = critic(obs, current_Q)\n",
    "          critic.values.append(predicted_value)\n",
    "          steps += 1\n",
    "        running_reward = 0.05 * InstantReward + (1 - 0.05) * running_reward\n",
    "        #print(env.routes)\n",
    "      # Let's finish the batch by updating the return\n",
    "      R = 0\n",
    "      for i in range(len(actor.rewards)):\n",
    "        if actor.dones[len(actor.rewards)-1-i] == True:\n",
    "          R = 0\n",
    "        R = actor.rewards[len(actor.rewards)-1-i] + discount*R\n",
    "        returns.insert(0,R)\n",
    "      \n",
    "      returns = torch.tensor(returns)\n",
    "      returns = (returns - torch.tensor(critic.values))\n",
    "\n",
    "      actor_loss = []\n",
    "      critic_loss = []\n",
    "      # Let's update the models\n",
    "      for log_prob, R in zip(actor.saved_log_probabilities, returns):\n",
    "        actor_loss.append(-log_prob * R)\n",
    "      self.actor_optimizer.zero_grad()\n",
    "      actor_loss = torch.cat(actor_loss).mean()\n",
    "      actor_loss.backward(retain_graph=True)\n",
    "      self.actor_optimizer.step()\n",
    "      del actor.rewards[:]\n",
    "      del actor.saved_log_probabilities[:]\n",
    "\n",
    "      for values, R in zip(critic.values, returns):\n",
    "        critic_loss.append((R-values)**2)\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss = torch.cat(critic_loss).mean()\n",
    "      critic_loss.backward(retain_graph=True)\n",
    "      self.critic_optimizer.step()\n",
    "      del critic.values[:]\n",
    "      \n",
    "      \n",
    "      print(\"Batch {} completed, Last reward: {:.2f} Average reward: {:.2f}\".format(it, InstantReward, running_reward))\n",
    "  \n",
    "  # Make a save method to save a trained model\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_actor_baseline.pth' % (directory, filename))\n",
    "  \n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_actor_baseline.pth' % (directory, filename)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ka-ZRtQvjBex"
   },
   "source": [
    "## Create a function that evaluates the policy by calculating its average reward over 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qabqiYdp9wDM"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    obs = torch.Tensor(obs.reshape(1,env.customer_count,state_dim)).to(device)\n",
    "    done = False\n",
    "    while not done:\n",
    "      action, current_Q = policy.select_target_action(obs, state_dim)\n",
    "      obs, reward, done = env.step(action)\n",
    "      obs = torch.Tensor(obs.reshape(1,env.customer_count,state_dim)).to(device)\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGuKmH_ijf7U"
   },
   "source": [
    "## Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HFj6wbAo97lk"
   },
   "outputs": [],
   "source": [
    "env_name = \"CVRP\" # Name of a environment (set it to any Continous environment you want)\n",
    "seed = 0 # Random seed number\n",
    "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
    "max_timesteps = 1000 # Total number of iterations/timesteps\n",
    "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
    "batch_size = 128 # Size of the batch\n",
    "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.001 # Target network update rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hjwf2HCol3XP"
   },
   "source": [
    "## Create a file name for the two saved models: the Actor and Critic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fyH8N5z-o3o",
    "outputId": "06fcefe4-5b94-4663-d318-35846eb6a543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: Actor_Critic_CVRP_0\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s_%s\" % (\"Actor_Critic\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kop-C96Aml8O"
   },
   "source": [
    "## Create a folder to save the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Src07lvY-zXb"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./results\"):\n",
    "  os.makedirs(\"./results\")\n",
    "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
    "  os.makedirs(\"./pytorch_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEAzOd47mv1Z"
   },
   "source": [
    "## Create an instance of the CVRP environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CyQXJUIs-6BV"
   },
   "outputs": [],
   "source": [
    "env = VRPEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YdPG4HXnNsh"
   },
   "source": [
    "## Set seeds and get the necessary information on the states and actions in the chosen environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Z3RufYec_ADj"
   },
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.customer_count\n",
    "max_action = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWEgDAQxnbem"
   },
   "source": [
    "## Create the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTVvG7F8_EWg",
    "outputId": "6a3056b2-8d70-44ce-eb5a-f8a7cc61cb6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 11 1\n"
     ]
    }
   ],
   "source": [
    "print(state_dim, action_dim, max_action)\n",
    "policy = Actor_Critic(state_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZI60VN2Unklh"
   },
   "source": [
    "## Create the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sd-ZsdXR_LgV"
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYOpCyiDnw7s"
   },
   "source": [
    "## Define a list where all the evaluation results over 10 episodes are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhC_5XJ__Orp",
    "outputId": "330d0cc2-c68e-4c48-f018-393b8ffdb76f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -5.430767\n",
      "---------------------------------------\n",
      "[[0, 7, 5, 4, 6, 1, 0], [0, 10, 2, 9, 0], [0, 3, 8, 0]]\n"
     ]
    }
   ],
   "source": [
    "evaluations = [evaluate_policy(policy, eval_episodes=1)]\n",
    "print(env.routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6t3LV3qZaWkt",
    "outputId": "21f7fcb8-1594-4efb-b574-947f97916cad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 7, 5, 4, 6, 1, 0], [0, 10, 2, 9, 0], [0, 3, 8, 0]]\n",
      "[[0.50808735 0.41159729 0.         2.        ]\n",
      " [0.00320192 0.94616417 0.         2.        ]\n",
      " [0.44781966 0.8304828  0.         2.        ]\n",
      " [0.2957106  0.99642091 0.         2.        ]\n",
      " [0.54220998 0.74679985 0.         2.        ]\n",
      " [0.46113452 0.37346651 0.         2.        ]\n",
      " [0.35870685 0.83994957 0.         2.        ]\n",
      " [0.74349391 0.34396168 0.         2.        ]\n",
      " [0.15516087 0.86054579 0.         2.        ]\n",
      " [0.07953033 0.69805703 0.         2.        ]\n",
      " [0.21270309 0.28743551 0.         2.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(env.routes)\n",
    "print(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xm-4b3p6rglE"
   },
   "source": [
    "## Create a folder directory in which the final results will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MTL9uMd0ru03"
   },
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "max_episode_steps = env._max_episode_steps\n",
    "save_env_vid = False\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31n5eb03p-Fm"
   },
   "source": [
    "## Initialize the training process variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1vN5EvxK_QhT"
   },
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9gsjvtPqLgT"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_ouY4NH_Y0I",
    "outputId": "0addd2b6-9b57-497f-827a-1e951af04e87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 completed, Last reward: -7.37 Average reward: -6.79\n",
      "Batch 1 completed, Last reward: -6.71 Average reward: -6.82\n",
      "Batch 2 completed, Last reward: -5.70 Average reward: -6.82\n",
      "Batch 3 completed, Last reward: -6.52 Average reward: -6.53\n",
      "Batch 4 completed, Last reward: -7.07 Average reward: -6.55\n",
      "Batch 5 completed, Last reward: -7.01 Average reward: -6.74\n",
      "Batch 6 completed, Last reward: -4.88 Average reward: -6.68\n",
      "Batch 7 completed, Last reward: -5.24 Average reward: -6.71\n",
      "Batch 8 completed, Last reward: -7.61 Average reward: -6.83\n",
      "Batch 9 completed, Last reward: -6.70 Average reward: -6.70\n",
      "Batch 10 completed, Last reward: -6.85 Average reward: -6.68\n",
      "Batch 11 completed, Last reward: -8.05 Average reward: -6.89\n",
      "Batch 12 completed, Last reward: -6.94 Average reward: -6.94\n",
      "Batch 13 completed, Last reward: -7.41 Average reward: -6.37\n",
      "Batch 14 completed, Last reward: -5.64 Average reward: -6.98\n",
      "Batch 15 completed, Last reward: -7.74 Average reward: -6.56\n",
      "Batch 16 completed, Last reward: -7.12 Average reward: -6.83\n",
      "Batch 17 completed, Last reward: -5.08 Average reward: -6.61\n",
      "Batch 18 completed, Last reward: -6.01 Average reward: -6.67\n",
      "Batch 19 completed, Last reward: -7.54 Average reward: -6.83\n",
      "Batch 20 completed, Last reward: -9.71 Average reward: -6.92\n",
      "Batch 21 completed, Last reward: -7.12 Average reward: -6.59\n",
      "Batch 22 completed, Last reward: -5.33 Average reward: -6.83\n",
      "Batch 23 completed, Last reward: -4.89 Average reward: -6.40\n",
      "Batch 24 completed, Last reward: -5.57 Average reward: -6.54\n",
      "Batch 25 completed, Last reward: -6.32 Average reward: -6.68\n",
      "Batch 26 completed, Last reward: -6.51 Average reward: -6.83\n",
      "Batch 27 completed, Last reward: -7.43 Average reward: -6.58\n",
      "Batch 28 completed, Last reward: -4.62 Average reward: -6.50\n",
      "Batch 29 completed, Last reward: -6.46 Average reward: -6.74\n",
      "Batch 30 completed, Last reward: -6.02 Average reward: -6.79\n",
      "Batch 31 completed, Last reward: -6.35 Average reward: -6.77\n",
      "Batch 32 completed, Last reward: -5.92 Average reward: -6.47\n",
      "Batch 33 completed, Last reward: -6.77 Average reward: -6.42\n",
      "Batch 34 completed, Last reward: -5.89 Average reward: -6.42\n",
      "Batch 35 completed, Last reward: -7.14 Average reward: -6.67\n",
      "Batch 36 completed, Last reward: -6.32 Average reward: -6.89\n",
      "Batch 37 completed, Last reward: -6.33 Average reward: -6.85\n",
      "Batch 38 completed, Last reward: -6.00 Average reward: -6.77\n",
      "Batch 39 completed, Last reward: -3.74 Average reward: -6.62\n",
      "Batch 40 completed, Last reward: -5.60 Average reward: -6.73\n",
      "Batch 41 completed, Last reward: -5.40 Average reward: -6.48\n",
      "Batch 42 completed, Last reward: -6.35 Average reward: -6.50\n",
      "Batch 43 completed, Last reward: -6.86 Average reward: -7.04\n",
      "Batch 44 completed, Last reward: -6.10 Average reward: -6.67\n",
      "Batch 45 completed, Last reward: -7.94 Average reward: -6.68\n",
      "Batch 46 completed, Last reward: -8.51 Average reward: -6.61\n",
      "Batch 47 completed, Last reward: -6.45 Average reward: -7.08\n",
      "Batch 48 completed, Last reward: -5.94 Average reward: -6.91\n",
      "Batch 49 completed, Last reward: -7.40 Average reward: -6.67\n",
      "Batch 50 completed, Last reward: -7.05 Average reward: -6.41\n",
      "Batch 51 completed, Last reward: -5.77 Average reward: -6.44\n",
      "Batch 52 completed, Last reward: -6.24 Average reward: -6.66\n",
      "Batch 53 completed, Last reward: -6.45 Average reward: -6.84\n",
      "Batch 54 completed, Last reward: -5.49 Average reward: -6.77\n",
      "Batch 55 completed, Last reward: -7.89 Average reward: -7.13\n",
      "Batch 56 completed, Last reward: -7.34 Average reward: -6.83\n",
      "Batch 57 completed, Last reward: -5.05 Average reward: -6.89\n",
      "Batch 58 completed, Last reward: -6.10 Average reward: -6.93\n",
      "Batch 59 completed, Last reward: -6.93 Average reward: -6.80\n",
      "Batch 60 completed, Last reward: -5.64 Average reward: -6.73\n",
      "Batch 61 completed, Last reward: -7.56 Average reward: -6.87\n",
      "Batch 62 completed, Last reward: -9.32 Average reward: -7.24\n",
      "Batch 63 completed, Last reward: -9.15 Average reward: -6.97\n",
      "Batch 64 completed, Last reward: -7.03 Average reward: -6.90\n",
      "Batch 65 completed, Last reward: -6.87 Average reward: -6.81\n",
      "Batch 66 completed, Last reward: -5.05 Average reward: -6.78\n",
      "Batch 67 completed, Last reward: -6.71 Average reward: -6.64\n",
      "Batch 68 completed, Last reward: -6.14 Average reward: -7.05\n",
      "Batch 69 completed, Last reward: -6.81 Average reward: -7.06\n",
      "Batch 70 completed, Last reward: -6.27 Average reward: -6.38\n",
      "Batch 71 completed, Last reward: -6.23 Average reward: -6.67\n",
      "Batch 72 completed, Last reward: -7.71 Average reward: -6.96\n",
      "Batch 73 completed, Last reward: -6.85 Average reward: -6.77\n",
      "Batch 74 completed, Last reward: -7.55 Average reward: -7.02\n",
      "Batch 75 completed, Last reward: -6.00 Average reward: -6.50\n",
      "Batch 76 completed, Last reward: -10.87 Average reward: -6.78\n",
      "Batch 77 completed, Last reward: -6.98 Average reward: -6.78\n",
      "Batch 78 completed, Last reward: -6.18 Average reward: -6.83\n",
      "Batch 79 completed, Last reward: -7.68 Average reward: -6.92\n",
      "Batch 80 completed, Last reward: -8.17 Average reward: -6.60\n",
      "Batch 81 completed, Last reward: -6.91 Average reward: -6.90\n",
      "Batch 82 completed, Last reward: -7.53 Average reward: -6.86\n",
      "Batch 83 completed, Last reward: -6.30 Average reward: -6.68\n",
      "Batch 84 completed, Last reward: -6.55 Average reward: -6.69\n",
      "Batch 85 completed, Last reward: -5.77 Average reward: -6.75\n",
      "Batch 86 completed, Last reward: -7.80 Average reward: -6.65\n",
      "Batch 87 completed, Last reward: -7.60 Average reward: -7.08\n",
      "Batch 88 completed, Last reward: -8.57 Average reward: -6.68\n",
      "Batch 89 completed, Last reward: -9.70 Average reward: -6.64\n",
      "Batch 90 completed, Last reward: -7.63 Average reward: -6.94\n",
      "Batch 91 completed, Last reward: -5.89 Average reward: -6.75\n",
      "Batch 92 completed, Last reward: -5.58 Average reward: -6.58\n",
      "Batch 93 completed, Last reward: -8.30 Average reward: -6.76\n",
      "Batch 94 completed, Last reward: -5.72 Average reward: -6.82\n",
      "Batch 95 completed, Last reward: -7.56 Average reward: -6.54\n",
      "Batch 96 completed, Last reward: -6.69 Average reward: -6.91\n",
      "Batch 97 completed, Last reward: -6.05 Average reward: -6.80\n",
      "Batch 98 completed, Last reward: -6.41 Average reward: -6.93\n",
      "Batch 99 completed, Last reward: -6.54 Average reward: -6.89\n",
      "Batch 100 completed, Last reward: -6.36 Average reward: -6.77\n",
      "Batch 101 completed, Last reward: -9.77 Average reward: -7.07\n",
      "Batch 102 completed, Last reward: -5.14 Average reward: -6.88\n",
      "Batch 103 completed, Last reward: -6.20 Average reward: -6.73\n",
      "Batch 104 completed, Last reward: -5.80 Average reward: -6.40\n",
      "Batch 105 completed, Last reward: -4.73 Average reward: -6.98\n",
      "Batch 106 completed, Last reward: -5.84 Average reward: -7.04\n",
      "Batch 107 completed, Last reward: -6.49 Average reward: -7.04\n",
      "Batch 108 completed, Last reward: -6.49 Average reward: -6.52\n",
      "Batch 109 completed, Last reward: -5.93 Average reward: -6.96\n",
      "Batch 110 completed, Last reward: -6.37 Average reward: -6.56\n",
      "Batch 111 completed, Last reward: -6.31 Average reward: -6.70\n",
      "Batch 112 completed, Last reward: -7.78 Average reward: -6.94\n",
      "Batch 113 completed, Last reward: -8.07 Average reward: -6.88\n",
      "Batch 114 completed, Last reward: -6.35 Average reward: -6.70\n",
      "Batch 115 completed, Last reward: -4.97 Average reward: -7.02\n",
      "Batch 116 completed, Last reward: -7.92 Average reward: -6.79\n",
      "Batch 117 completed, Last reward: -8.56 Average reward: -6.87\n",
      "Batch 118 completed, Last reward: -4.99 Average reward: -6.46\n",
      "Batch 119 completed, Last reward: -6.41 Average reward: -7.02\n",
      "Batch 120 completed, Last reward: -6.49 Average reward: -6.89\n",
      "Batch 121 completed, Last reward: -7.42 Average reward: -6.70\n",
      "Batch 122 completed, Last reward: -6.32 Average reward: -6.93\n",
      "Batch 123 completed, Last reward: -7.13 Average reward: -6.87\n",
      "Batch 124 completed, Last reward: -6.15 Average reward: -7.03\n",
      "Batch 125 completed, Last reward: -5.95 Average reward: -6.72\n",
      "Batch 126 completed, Last reward: -6.83 Average reward: -6.90\n",
      "Batch 127 completed, Last reward: -9.79 Average reward: -6.91\n",
      "Batch 128 completed, Last reward: -7.00 Average reward: -6.98\n",
      "Batch 129 completed, Last reward: -5.69 Average reward: -6.37\n",
      "Batch 130 completed, Last reward: -6.63 Average reward: -6.60\n",
      "Batch 131 completed, Last reward: -8.16 Average reward: -6.89\n",
      "Batch 132 completed, Last reward: -7.06 Average reward: -6.75\n",
      "Batch 133 completed, Last reward: -6.64 Average reward: -6.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 134 completed, Last reward: -7.12 Average reward: -6.82\n",
      "Batch 135 completed, Last reward: -5.41 Average reward: -6.83\n",
      "Batch 136 completed, Last reward: -5.06 Average reward: -6.55\n",
      "Batch 137 completed, Last reward: -8.91 Average reward: -6.94\n",
      "Batch 138 completed, Last reward: -5.25 Average reward: -7.01\n",
      "Batch 139 completed, Last reward: -6.82 Average reward: -6.66\n",
      "Batch 140 completed, Last reward: -7.51 Average reward: -6.72\n",
      "Batch 141 completed, Last reward: -4.83 Average reward: -6.66\n",
      "Batch 142 completed, Last reward: -8.98 Average reward: -6.63\n",
      "Batch 143 completed, Last reward: -7.07 Average reward: -6.61\n",
      "Batch 144 completed, Last reward: -5.91 Average reward: -6.75\n",
      "Batch 145 completed, Last reward: -5.62 Average reward: -6.53\n",
      "Batch 146 completed, Last reward: -6.19 Average reward: -7.00\n",
      "Batch 147 completed, Last reward: -6.42 Average reward: -6.50\n",
      "Batch 148 completed, Last reward: -6.67 Average reward: -6.88\n",
      "Batch 149 completed, Last reward: -7.50 Average reward: -6.72\n",
      "Batch 150 completed, Last reward: -3.79 Average reward: -6.62\n",
      "Batch 151 completed, Last reward: -7.69 Average reward: -6.59\n",
      "Batch 152 completed, Last reward: -6.23 Average reward: -6.70\n",
      "Batch 153 completed, Last reward: -7.24 Average reward: -6.82\n",
      "Batch 154 completed, Last reward: -5.68 Average reward: -6.94\n",
      "Batch 155 completed, Last reward: -6.22 Average reward: -6.74\n",
      "Batch 156 completed, Last reward: -6.52 Average reward: -6.59\n",
      "Batch 157 completed, Last reward: -9.90 Average reward: -6.74\n",
      "Batch 158 completed, Last reward: -6.41 Average reward: -6.77\n",
      "Batch 159 completed, Last reward: -8.27 Average reward: -6.70\n",
      "Batch 160 completed, Last reward: -6.06 Average reward: -6.64\n",
      "Batch 161 completed, Last reward: -6.76 Average reward: -6.89\n",
      "Batch 162 completed, Last reward: -6.51 Average reward: -6.85\n",
      "Batch 163 completed, Last reward: -8.73 Average reward: -6.89\n",
      "Batch 164 completed, Last reward: -6.73 Average reward: -6.77\n",
      "Batch 165 completed, Last reward: -8.45 Average reward: -6.62\n",
      "Batch 166 completed, Last reward: -7.00 Average reward: -6.71\n",
      "Batch 167 completed, Last reward: -6.79 Average reward: -6.63\n",
      "Batch 168 completed, Last reward: -6.10 Average reward: -6.82\n",
      "Batch 169 completed, Last reward: -5.01 Average reward: -6.80\n",
      "Batch 170 completed, Last reward: -8.59 Average reward: -6.71\n",
      "Batch 171 completed, Last reward: -6.38 Average reward: -6.70\n",
      "Batch 172 completed, Last reward: -7.27 Average reward: -6.44\n",
      "Batch 173 completed, Last reward: -6.80 Average reward: -6.63\n",
      "Batch 174 completed, Last reward: -6.26 Average reward: -6.74\n",
      "Batch 175 completed, Last reward: -8.86 Average reward: -6.83\n",
      "Batch 176 completed, Last reward: -5.18 Average reward: -6.61\n",
      "Batch 177 completed, Last reward: -7.19 Average reward: -6.42\n",
      "Batch 178 completed, Last reward: -7.27 Average reward: -6.53\n",
      "Batch 179 completed, Last reward: -7.60 Average reward: -6.72\n",
      "Batch 180 completed, Last reward: -6.62 Average reward: -6.58\n",
      "Batch 181 completed, Last reward: -6.63 Average reward: -7.01\n",
      "Batch 182 completed, Last reward: -6.84 Average reward: -6.62\n",
      "Batch 183 completed, Last reward: -7.29 Average reward: -6.65\n",
      "Batch 184 completed, Last reward: -8.20 Average reward: -6.93\n",
      "Batch 185 completed, Last reward: -6.21 Average reward: -7.00\n",
      "Batch 186 completed, Last reward: -8.49 Average reward: -6.72\n",
      "Batch 187 completed, Last reward: -7.51 Average reward: -7.10\n",
      "Batch 188 completed, Last reward: -7.68 Average reward: -7.12\n",
      "Batch 189 completed, Last reward: -4.67 Average reward: -6.86\n",
      "Batch 190 completed, Last reward: -4.15 Average reward: -6.68\n",
      "Batch 191 completed, Last reward: -5.56 Average reward: -6.50\n",
      "Batch 192 completed, Last reward: -6.51 Average reward: -6.80\n",
      "Batch 193 completed, Last reward: -5.61 Average reward: -6.60\n",
      "Batch 194 completed, Last reward: -6.05 Average reward: -6.79\n",
      "Batch 195 completed, Last reward: -6.50 Average reward: -6.79\n",
      "Batch 196 completed, Last reward: -8.28 Average reward: -7.01\n",
      "Batch 197 completed, Last reward: -6.35 Average reward: -6.68\n",
      "Batch 198 completed, Last reward: -8.36 Average reward: -6.66\n",
      "Batch 199 completed, Last reward: -4.91 Average reward: -6.63\n",
      "Batch 200 completed, Last reward: -6.01 Average reward: -6.67\n",
      "Batch 201 completed, Last reward: -6.81 Average reward: -6.96\n",
      "Batch 202 completed, Last reward: -6.71 Average reward: -6.89\n",
      "Batch 203 completed, Last reward: -7.10 Average reward: -6.59\n",
      "Batch 204 completed, Last reward: -7.80 Average reward: -6.78\n",
      "Batch 205 completed, Last reward: -7.26 Average reward: -6.70\n",
      "Batch 206 completed, Last reward: -7.46 Average reward: -6.78\n",
      "Batch 207 completed, Last reward: -7.03 Average reward: -6.54\n",
      "Batch 208 completed, Last reward: -6.58 Average reward: -6.80\n",
      "Batch 209 completed, Last reward: -5.65 Average reward: -6.91\n",
      "Batch 210 completed, Last reward: -6.80 Average reward: -7.01\n",
      "Batch 211 completed, Last reward: -4.97 Average reward: -6.94\n",
      "Batch 212 completed, Last reward: -6.39 Average reward: -6.54\n",
      "Batch 213 completed, Last reward: -6.92 Average reward: -6.53\n",
      "Batch 214 completed, Last reward: -7.90 Average reward: -6.98\n",
      "Batch 215 completed, Last reward: -8.17 Average reward: -6.69\n",
      "Batch 216 completed, Last reward: -6.04 Average reward: -6.55\n",
      "Batch 217 completed, Last reward: -6.94 Average reward: -6.71\n",
      "Batch 218 completed, Last reward: -6.59 Average reward: -6.54\n",
      "Batch 219 completed, Last reward: -5.26 Average reward: -6.64\n",
      "Batch 220 completed, Last reward: -6.81 Average reward: -7.20\n",
      "Batch 221 completed, Last reward: -6.20 Average reward: -6.97\n",
      "Batch 222 completed, Last reward: -7.38 Average reward: -6.85\n",
      "Batch 223 completed, Last reward: -7.18 Average reward: -6.52\n",
      "Batch 224 completed, Last reward: -8.55 Average reward: -6.85\n",
      "Batch 225 completed, Last reward: -8.47 Average reward: -6.98\n",
      "Batch 226 completed, Last reward: -5.30 Average reward: -6.84\n",
      "Batch 227 completed, Last reward: -4.32 Average reward: -6.85\n",
      "Batch 228 completed, Last reward: -7.60 Average reward: -6.83\n",
      "Batch 229 completed, Last reward: -6.01 Average reward: -6.78\n",
      "Batch 230 completed, Last reward: -6.75 Average reward: -6.49\n",
      "Batch 231 completed, Last reward: -8.94 Average reward: -6.88\n",
      "Batch 232 completed, Last reward: -7.78 Average reward: -6.97\n",
      "Batch 233 completed, Last reward: -5.96 Average reward: -6.75\n",
      "Batch 234 completed, Last reward: -5.42 Average reward: -6.79\n",
      "Batch 235 completed, Last reward: -7.69 Average reward: -6.86\n",
      "Batch 236 completed, Last reward: -8.58 Average reward: -7.03\n",
      "Batch 237 completed, Last reward: -7.50 Average reward: -6.83\n",
      "Batch 238 completed, Last reward: -7.42 Average reward: -6.59\n",
      "Batch 239 completed, Last reward: -6.56 Average reward: -6.88\n",
      "Batch 240 completed, Last reward: -7.78 Average reward: -6.69\n",
      "Batch 241 completed, Last reward: -7.16 Average reward: -6.74\n",
      "Batch 242 completed, Last reward: -5.26 Average reward: -6.90\n",
      "Batch 243 completed, Last reward: -6.81 Average reward: -6.70\n",
      "Batch 244 completed, Last reward: -6.71 Average reward: -7.11\n",
      "Batch 245 completed, Last reward: -7.94 Average reward: -6.57\n",
      "Batch 246 completed, Last reward: -8.18 Average reward: -6.89\n",
      "Batch 247 completed, Last reward: -5.27 Average reward: -6.48\n",
      "Batch 248 completed, Last reward: -5.39 Average reward: -6.33\n",
      "Batch 249 completed, Last reward: -5.88 Average reward: -6.73\n",
      "Batch 250 completed, Last reward: -7.54 Average reward: -6.84\n",
      "Batch 251 completed, Last reward: -7.02 Average reward: -6.83\n",
      "Batch 252 completed, Last reward: -6.23 Average reward: -6.62\n",
      "Batch 253 completed, Last reward: -5.78 Average reward: -6.43\n",
      "Batch 254 completed, Last reward: -5.31 Average reward: -6.66\n",
      "Batch 255 completed, Last reward: -5.81 Average reward: -6.52\n",
      "Batch 256 completed, Last reward: -6.78 Average reward: -6.66\n",
      "Batch 257 completed, Last reward: -6.42 Average reward: -6.65\n",
      "Batch 258 completed, Last reward: -6.40 Average reward: -6.79\n",
      "Batch 259 completed, Last reward: -7.72 Average reward: -6.76\n",
      "Batch 260 completed, Last reward: -6.90 Average reward: -6.82\n",
      "Batch 261 completed, Last reward: -5.83 Average reward: -6.85\n",
      "Batch 262 completed, Last reward: -6.12 Average reward: -6.59\n",
      "Batch 263 completed, Last reward: -8.73 Average reward: -7.04\n",
      "Batch 264 completed, Last reward: -7.44 Average reward: -6.89\n",
      "Batch 265 completed, Last reward: -6.99 Average reward: -6.81\n",
      "Batch 266 completed, Last reward: -6.89 Average reward: -6.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 267 completed, Last reward: -7.31 Average reward: -6.74\n",
      "Batch 268 completed, Last reward: -6.42 Average reward: -6.65\n",
      "Batch 269 completed, Last reward: -7.59 Average reward: -7.22\n",
      "Batch 270 completed, Last reward: -7.36 Average reward: -6.85\n",
      "Batch 271 completed, Last reward: -4.72 Average reward: -6.84\n",
      "Batch 272 completed, Last reward: -5.46 Average reward: -6.59\n",
      "Batch 273 completed, Last reward: -7.11 Average reward: -6.63\n",
      "Batch 274 completed, Last reward: -6.05 Average reward: -6.70\n",
      "Batch 275 completed, Last reward: -7.46 Average reward: -6.66\n",
      "Batch 276 completed, Last reward: -6.52 Average reward: -6.93\n",
      "Batch 277 completed, Last reward: -5.85 Average reward: -6.76\n",
      "Batch 278 completed, Last reward: -5.42 Average reward: -6.79\n",
      "Batch 279 completed, Last reward: -7.08 Average reward: -6.66\n",
      "Batch 280 completed, Last reward: -7.55 Average reward: -6.55\n",
      "Batch 281 completed, Last reward: -6.33 Average reward: -7.09\n",
      "Batch 282 completed, Last reward: -6.26 Average reward: -6.64\n",
      "Batch 283 completed, Last reward: -6.67 Average reward: -6.77\n",
      "Batch 284 completed, Last reward: -6.32 Average reward: -7.17\n",
      "Batch 285 completed, Last reward: -7.96 Average reward: -7.00\n",
      "Batch 286 completed, Last reward: -4.76 Average reward: -6.33\n",
      "Batch 287 completed, Last reward: -7.70 Average reward: -6.82\n",
      "Batch 288 completed, Last reward: -7.94 Average reward: -6.60\n",
      "Batch 289 completed, Last reward: -6.76 Average reward: -6.74\n",
      "Batch 290 completed, Last reward: -6.17 Average reward: -6.72\n",
      "Batch 291 completed, Last reward: -7.10 Average reward: -6.71\n",
      "Batch 292 completed, Last reward: -8.97 Average reward: -7.02\n",
      "Batch 293 completed, Last reward: -6.68 Average reward: -6.82\n",
      "Batch 294 completed, Last reward: -7.53 Average reward: -6.99\n",
      "Batch 295 completed, Last reward: -8.78 Average reward: -6.72\n",
      "Batch 296 completed, Last reward: -7.92 Average reward: -6.59\n",
      "Batch 297 completed, Last reward: -5.16 Average reward: -6.48\n",
      "Batch 298 completed, Last reward: -6.63 Average reward: -6.95\n",
      "Batch 299 completed, Last reward: -8.17 Average reward: -6.70\n",
      "Batch 300 completed, Last reward: -9.31 Average reward: -6.63\n",
      "Batch 301 completed, Last reward: -5.71 Average reward: -6.51\n",
      "Batch 302 completed, Last reward: -6.46 Average reward: -6.83\n",
      "Batch 303 completed, Last reward: -5.79 Average reward: -6.72\n",
      "Batch 304 completed, Last reward: -8.15 Average reward: -6.87\n",
      "Batch 305 completed, Last reward: -9.52 Average reward: -7.24\n",
      "Batch 306 completed, Last reward: -6.64 Average reward: -6.73\n",
      "Batch 307 completed, Last reward: -8.18 Average reward: -6.85\n",
      "Batch 308 completed, Last reward: -7.37 Average reward: -6.78\n",
      "Batch 309 completed, Last reward: -5.21 Average reward: -6.51\n",
      "Batch 310 completed, Last reward: -6.75 Average reward: -6.70\n",
      "Batch 311 completed, Last reward: -5.95 Average reward: -6.81\n",
      "Batch 312 completed, Last reward: -6.00 Average reward: -6.66\n",
      "Batch 313 completed, Last reward: -8.34 Average reward: -6.94\n",
      "Batch 314 completed, Last reward: -5.40 Average reward: -6.83\n",
      "Batch 315 completed, Last reward: -5.12 Average reward: -6.53\n",
      "Batch 316 completed, Last reward: -6.41 Average reward: -6.72\n",
      "Batch 317 completed, Last reward: -5.37 Average reward: -6.91\n",
      "Batch 318 completed, Last reward: -6.85 Average reward: -6.86\n",
      "Batch 319 completed, Last reward: -5.62 Average reward: -6.63\n",
      "Batch 320 completed, Last reward: -5.70 Average reward: -6.88\n",
      "Batch 321 completed, Last reward: -6.64 Average reward: -6.83\n",
      "Batch 322 completed, Last reward: -5.21 Average reward: -6.76\n",
      "Batch 323 completed, Last reward: -4.84 Average reward: -6.45\n",
      "Batch 324 completed, Last reward: -6.00 Average reward: -6.44\n",
      "Batch 325 completed, Last reward: -7.41 Average reward: -6.60\n",
      "Batch 326 completed, Last reward: -5.87 Average reward: -6.70\n",
      "Batch 327 completed, Last reward: -6.40 Average reward: -6.82\n",
      "Batch 328 completed, Last reward: -7.29 Average reward: -6.63\n",
      "Batch 329 completed, Last reward: -5.60 Average reward: -6.77\n",
      "Batch 330 completed, Last reward: -6.04 Average reward: -6.93\n",
      "Batch 331 completed, Last reward: -7.97 Average reward: -6.48\n",
      "Batch 332 completed, Last reward: -5.90 Average reward: -6.61\n",
      "Batch 333 completed, Last reward: -6.41 Average reward: -6.62\n",
      "Batch 334 completed, Last reward: -7.76 Average reward: -6.65\n",
      "Batch 335 completed, Last reward: -5.76 Average reward: -6.97\n",
      "Batch 336 completed, Last reward: -7.54 Average reward: -6.94\n",
      "Batch 337 completed, Last reward: -7.12 Average reward: -6.83\n",
      "Batch 338 completed, Last reward: -7.79 Average reward: -6.82\n",
      "Batch 339 completed, Last reward: -7.54 Average reward: -6.52\n",
      "Batch 340 completed, Last reward: -6.86 Average reward: -6.41\n",
      "Batch 341 completed, Last reward: -4.65 Average reward: -6.71\n",
      "Batch 342 completed, Last reward: -6.82 Average reward: -6.75\n",
      "Batch 343 completed, Last reward: -6.57 Average reward: -6.68\n",
      "Batch 344 completed, Last reward: -4.89 Average reward: -6.59\n",
      "Batch 345 completed, Last reward: -5.34 Average reward: -6.88\n",
      "Batch 346 completed, Last reward: -7.80 Average reward: -6.68\n",
      "Batch 347 completed, Last reward: -5.70 Average reward: -6.99\n",
      "Batch 348 completed, Last reward: -7.07 Average reward: -6.82\n",
      "Batch 349 completed, Last reward: -6.39 Average reward: -6.68\n",
      "Batch 350 completed, Last reward: -7.76 Average reward: -6.72\n",
      "Batch 351 completed, Last reward: -7.00 Average reward: -6.70\n",
      "Batch 352 completed, Last reward: -6.30 Average reward: -6.87\n",
      "Batch 353 completed, Last reward: -6.16 Average reward: -6.59\n",
      "Batch 354 completed, Last reward: -6.88 Average reward: -6.78\n",
      "Batch 355 completed, Last reward: -7.04 Average reward: -6.53\n",
      "Batch 356 completed, Last reward: -6.21 Average reward: -6.37\n",
      "Batch 357 completed, Last reward: -7.48 Average reward: -6.70\n",
      "Batch 358 completed, Last reward: -7.97 Average reward: -6.74\n",
      "Batch 359 completed, Last reward: -6.73 Average reward: -7.03\n",
      "Batch 360 completed, Last reward: -5.90 Average reward: -6.56\n",
      "Batch 361 completed, Last reward: -7.10 Average reward: -6.65\n",
      "Batch 362 completed, Last reward: -8.66 Average reward: -6.95\n",
      "Batch 363 completed, Last reward: -6.20 Average reward: -6.89\n",
      "Batch 364 completed, Last reward: -4.63 Average reward: -6.31\n",
      "Batch 365 completed, Last reward: -6.66 Average reward: -6.70\n",
      "Batch 366 completed, Last reward: -6.02 Average reward: -6.71\n",
      "Batch 367 completed, Last reward: -5.70 Average reward: -6.76\n",
      "Batch 368 completed, Last reward: -9.26 Average reward: -6.99\n",
      "Batch 369 completed, Last reward: -6.72 Average reward: -6.77\n",
      "Batch 370 completed, Last reward: -7.21 Average reward: -6.93\n",
      "Batch 371 completed, Last reward: -5.86 Average reward: -6.33\n",
      "Batch 372 completed, Last reward: -5.71 Average reward: -6.80\n",
      "Batch 373 completed, Last reward: -5.87 Average reward: -6.75\n",
      "Batch 374 completed, Last reward: -8.63 Average reward: -6.40\n",
      "Batch 375 completed, Last reward: -8.27 Average reward: -6.56\n",
      "Batch 376 completed, Last reward: -7.12 Average reward: -6.59\n",
      "Batch 377 completed, Last reward: -5.83 Average reward: -6.68\n",
      "Batch 378 completed, Last reward: -6.72 Average reward: -6.64\n",
      "Batch 379 completed, Last reward: -7.57 Average reward: -6.64\n",
      "Batch 380 completed, Last reward: -6.67 Average reward: -6.66\n",
      "Batch 381 completed, Last reward: -6.53 Average reward: -6.70\n",
      "Batch 382 completed, Last reward: -5.81 Average reward: -6.93\n",
      "Batch 383 completed, Last reward: -9.15 Average reward: -6.80\n",
      "Batch 384 completed, Last reward: -6.10 Average reward: -6.88\n",
      "Batch 385 completed, Last reward: -5.41 Average reward: -6.39\n",
      "Batch 386 completed, Last reward: -4.93 Average reward: -6.47\n",
      "Batch 387 completed, Last reward: -7.51 Average reward: -6.87\n",
      "Batch 388 completed, Last reward: -8.02 Average reward: -7.20\n",
      "Batch 389 completed, Last reward: -6.65 Average reward: -6.95\n",
      "Batch 390 completed, Last reward: -6.65 Average reward: -6.56\n",
      "Batch 391 completed, Last reward: -7.02 Average reward: -6.88\n",
      "Batch 392 completed, Last reward: -7.36 Average reward: -6.76\n",
      "Batch 393 completed, Last reward: -4.30 Average reward: -6.33\n",
      "Batch 394 completed, Last reward: -7.73 Average reward: -6.92\n",
      "Batch 395 completed, Last reward: -6.05 Average reward: -6.57\n",
      "Batch 396 completed, Last reward: -7.96 Average reward: -6.87\n",
      "Batch 397 completed, Last reward: -6.46 Average reward: -6.99\n",
      "Batch 398 completed, Last reward: -6.75 Average reward: -6.99\n",
      "Batch 399 completed, Last reward: -6.71 Average reward: -6.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400 completed, Last reward: -7.81 Average reward: -6.85\n",
      "Batch 401 completed, Last reward: -7.16 Average reward: -6.52\n",
      "Batch 402 completed, Last reward: -6.42 Average reward: -6.65\n",
      "Batch 403 completed, Last reward: -6.09 Average reward: -6.64\n",
      "Batch 404 completed, Last reward: -7.12 Average reward: -6.58\n",
      "Batch 405 completed, Last reward: -6.13 Average reward: -6.61\n",
      "Batch 406 completed, Last reward: -9.49 Average reward: -7.26\n",
      "Batch 407 completed, Last reward: -4.63 Average reward: -6.64\n",
      "Batch 408 completed, Last reward: -4.56 Average reward: -6.76\n",
      "Batch 409 completed, Last reward: -7.73 Average reward: -6.53\n",
      "Batch 410 completed, Last reward: -5.86 Average reward: -6.56\n",
      "Batch 411 completed, Last reward: -5.37 Average reward: -6.73\n",
      "Batch 412 completed, Last reward: -7.33 Average reward: -6.69\n",
      "Batch 413 completed, Last reward: -7.84 Average reward: -6.79\n",
      "Batch 414 completed, Last reward: -4.44 Average reward: -6.41\n",
      "Batch 415 completed, Last reward: -7.57 Average reward: -6.89\n",
      "Batch 416 completed, Last reward: -9.21 Average reward: -6.89\n",
      "Batch 417 completed, Last reward: -6.06 Average reward: -6.78\n",
      "Batch 418 completed, Last reward: -6.27 Average reward: -6.75\n",
      "Batch 419 completed, Last reward: -6.48 Average reward: -6.78\n",
      "Batch 420 completed, Last reward: -5.03 Average reward: -6.58\n",
      "Batch 421 completed, Last reward: -7.65 Average reward: -6.57\n",
      "Batch 422 completed, Last reward: -8.61 Average reward: -6.61\n",
      "Batch 423 completed, Last reward: -6.26 Average reward: -6.78\n",
      "Batch 424 completed, Last reward: -7.05 Average reward: -6.80\n",
      "Batch 425 completed, Last reward: -4.74 Average reward: -6.75\n",
      "Batch 426 completed, Last reward: -6.38 Average reward: -7.03\n",
      "Batch 427 completed, Last reward: -8.13 Average reward: -6.91\n",
      "Batch 428 completed, Last reward: -8.30 Average reward: -6.94\n",
      "Batch 429 completed, Last reward: -5.96 Average reward: -6.47\n",
      "Batch 430 completed, Last reward: -6.93 Average reward: -6.55\n",
      "Batch 431 completed, Last reward: -8.03 Average reward: -6.67\n"
     ]
    }
   ],
   "source": [
    "# We start the main loop over 500,000 timesteps\n",
    "env.reset()\n",
    "total_timesteps = 0\n",
    "obs = copy.deepcopy(env.reset())\n",
    "\n",
    "policy.train(int(max_timesteps), batch_size, tau)\n",
    "\n",
    "# Add the last policy evaluation to our list of evaluations and we save our model\n",
    "evaluations.append(evaluate_policy(policy))\n",
    "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
    "np.save(\"./results/%s\" % (file_name), evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZT4EQ6XJi6h"
   },
   "outputs": [],
   "source": [
    "print(env.routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcB82cJup6kp"
   },
   "source": [
    "## The inference policy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUIPg-FCp-fG"
   },
   "outputs": [],
   "source": [
    "def evaluate_final_policy(policy,random = 1, eval_episodes=1):\n",
    "  avg_reward = 0.\n",
    "  distance = 0.\n",
    "  for k in range(eval_episodes):\n",
    "    obs = env.reset(seed = random)\n",
    "    obs = torch.Tensor(obs.reshape(1,env.customer_count,state_dim)).to(device)\n",
    "    done = False\n",
    "    while not done:\n",
    "      action, current_Q = policy.select_target_action(obs, state_dim)\n",
    "      obs, reward, done = env.step(action)\n",
    "      obs = torch.Tensor(obs.reshape(1,env.customer_count,state_dim)).to(device)\n",
    "      avg_reward += reward\n",
    "    total_distance = 0\n",
    "    for j in range(len(env.routes)):\n",
    "      for i in range(len(env.routes[j])-1):\n",
    "        total_distance = total_distance + ((env.VRP[env.routes[j][i],0]-env.VRP[env.routes[j][i+1],0])**2+(env.VRP[env.routes[j][i],1]-env.VRP[env.routes[j][i+1],1])**2)**0.5\n",
    "    distance += total_distance\n",
    "  distance /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Distance over the Evaluation Step: %f\" % (distance))\n",
    "  print (\"---------------------------------------\")\n",
    "  return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZbeSFVKo1rd"
   },
   "source": [
    "## Let's see if we can test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Edox0ADo64E"
   },
   "outputs": [],
   "source": [
    "results = np.zeros(100)\n",
    "#policy.load(file_name, './pytorch_models/')\n",
    "for i in range(100):\n",
    "  evaluations = [evaluate_final_policy(policy, random = i)]\n",
    "  results[i] = evaluations[0]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dt7nyqnr0maZ"
   },
   "outputs": [],
   "source": [
    "print(results.mean())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CartrackReinforceAttempt_V1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "KERNEL_DISPLAY_NAME",
   "language": "python",
   "name": "environment_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
