{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartrackRLAttempt_V3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf"
      },
      "source": [
        "# Actor-critic learning for the CVRP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2mkXSZ-JWOr"
      },
      "source": [
        "## Creating a CVRP environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A98HsVVJbT5"
      },
      "source": [
        "import gym\n",
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import math\n",
        "\n",
        "class VRPEnv(gym.Env):\n",
        "  def __init__(self):\n",
        "    # customer count ('0' is depot) \n",
        "    self.customer_count = 11\n",
        "    # the capacity of vehicles\n",
        "    self.vehicle_capacity = 2\n",
        "  \n",
        "    self.action_space = spaces.Discrete(3)\n",
        "    self.observation_space = spaces.Box(low=0,high=1, shape=(4,1), dtype=np.float64)\n",
        "    self.VRP = np.array((self.customer_count,4))\n",
        "    self._max_episode_steps = 1000\n",
        "    self.viewer = None\n",
        "    self.state = None\n",
        "    self.steps_beyond_done = None\n",
        "    self.route = []\n",
        "    self.route.append(0)\n",
        "    self.previous_action = 0\n",
        "    self.hn_actor = torch.zeros([1,self.customer_count,128], dtype=torch.float32)\n",
        "    \n",
        "\n",
        "  def reset(self, seed=200):\n",
        "    if seed == 200:\n",
        "      seed = int(time.time())\n",
        "    np.random.seed(seed)\n",
        "    x_locations = (np.random.rand(self.customer_count)).reshape((self.customer_count,1))\n",
        "    y_locations = (np.random.rand(self.customer_count)).reshape((self.customer_count,1))\n",
        "    demand = (np.random.randint(1,9,self.customer_count).reshape((self.customer_count,1))).reshape((self.customer_count,1))/10 # Normalise to between 0.1 and 0.9\n",
        "    capacity = np.repeat(self.vehicle_capacity,self.customer_count).reshape((self.customer_count,1))\n",
        "    VRP = np.concatenate((np.concatenate((np.concatenate((x_locations,y_locations), axis=1),demand),axis=1),capacity),axis=1)\n",
        "    self.VRP = VRP.reshape((self.customer_count,4))\n",
        "    self.unserved_customers = []\n",
        "    for i in range(1, self.customer_count):\n",
        "      self.unserved_customers.append(i)\n",
        "    self.routes = []\n",
        "    self.route = []\n",
        "    self.route.append(0)\n",
        "    self.VRP[0,2] = 0 # Set the demand at thedepot to 0\n",
        "    self.state = copy.deepcopy(self.VRP)\n",
        "    self.previous_action = 0\n",
        "    return self.state\n",
        "  \n",
        "\n",
        "  def step(self, action):\n",
        "    # Calculate the reward as the negative euclidean distance\n",
        "    reward = -((self.state[self.previous_action,0]-self.state[action,0])**2+(self.state[self.previous_action,1]-self.state[action,1])**2)**0.5 # - Euclidean distance between customers\n",
        "    load = self.state[0,3]\n",
        "    self.state[:,3] = max(0,(load-self.state[action,2])) # Update the vehicle load\n",
        "    self.state[action, 2] = max(0,self.state[action,2]-load) # Update the demand at served customer\n",
        "    done = False\n",
        "    if action == 0: # Return to the depot\n",
        "      self.route.append(action) # End route\n",
        "      self.routes.append(self.route) # Add subroute to list of all routes\n",
        "      self.route = [] # Initiate new subroute\n",
        "      self.state[:,3] = self.vehicle_capacity # Refill the vehicle\n",
        "    self.route.append(action) # Add action to the subroute\n",
        "    if max(self.state[:,2]) > 0: # If there are unserved customers left\n",
        "      done = False\n",
        "    elif max(self.state[:,2]) == 0 and action == 0: # If there are no unserved customers left and we have returned to the depot\n",
        "      done = True\n",
        "      self.route.append(0)\n",
        "    self.previous_action = action # Update the previous action\n",
        "    return self.state, reward, done\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyy2xkZRNCRw"
      },
      "source": [
        "## Let's test the environment step function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_xu5wGbNINA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98de899c-a386-4aad-d400-6e4852673eef"
      },
      "source": [
        "env = VRPEnv() # Create an instance of the environment\n",
        "state = env.reset() # Reset the environment\n",
        "action = 2 # Perform action with customer 2\n",
        "print(state)\n",
        "state, reward, done = env.step(action) # Perform the actual transition\n",
        "print(state)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.14920626 0.95583498 0.         2.        ]\n",
            " [0.64831157 0.02222461 0.5        2.        ]\n",
            " [0.63415072 0.50040921 0.7        2.        ]\n",
            " [0.42254174 0.94408989 0.4        2.        ]\n",
            " [0.13045372 0.89656329 0.4        2.        ]\n",
            " [0.82710122 0.48264696 0.4        2.        ]\n",
            " [0.4998244  0.06814091 0.4        2.        ]\n",
            " [0.38068211 0.88687239 0.4        2.        ]\n",
            " [0.59348622 0.36904166 0.2        2.        ]\n",
            " [0.2277177  0.64766929 0.4        2.        ]\n",
            " [0.67711674 0.64106762 0.6        2.        ]]\n",
            "[[0.14920626 0.95583498 0.         1.3       ]\n",
            " [0.64831157 0.02222461 0.5        1.3       ]\n",
            " [0.63415072 0.50040921 0.         1.3       ]\n",
            " [0.42254174 0.94408989 0.4        1.3       ]\n",
            " [0.13045372 0.89656329 0.4        1.3       ]\n",
            " [0.82710122 0.48264696 0.4        1.3       ]\n",
            " [0.4998244  0.06814091 0.4        1.3       ]\n",
            " [0.38068211 0.88687239 0.4        1.3       ]\n",
            " [0.59348622 0.36904166 0.2        1.3       ]\n",
            " [0.2277177  0.64766929 0.4        1.3       ]\n",
            " [0.67711674 0.64106762 0.6        1.3       ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr"
      },
      "source": [
        "## Initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD"
      },
      "source": [
        "## Build the neural network for the Actor and Actor-target models - contains the attention mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim=4, embed_size = 128):#, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.embed = nn.Linear((state_dim), embed_size) # Encoding to higher dimensional space - can also be changed to convolutional layer as in the paper\n",
        "    self.u_t = nn.RNN(embed_size,embed_size,1) # RNN Layer for the attention mechanism\n",
        "    self.v_t_a = nn.Linear(embed_size,1) # Linear for getting u_t\n",
        "    self.bar_u_t = nn.RNN(embed_size,embed_size,1) # RNN Layer for the context vector\n",
        "    self.a_t = nn.Softmax(dim = 1) # Softmax layer for the attention mechanism\n",
        "    self.v_t_u = nn.Linear(embed_size,1) # Linear for getting u_t\n",
        "    self.final = nn.Softmax(dim = 1) # Softmax layer for the final output\n",
        "\n",
        "  def forward(self, x, hn = env.hn_actor):\n",
        "    cond1 = (x[:,:,2]<x[:,:,3]).int() # Can we meet the demand\n",
        "    cond2 = (x[:,:,2]>0).int() # Is there demand at the customer\n",
        "    mask1 = torch.minimum(cond1,cond2) # Select those customers with demand, and whose demand we can meet\n",
        "    mask1 = torch.reshape(mask1,(len(x),env.customer_count,1))\n",
        "    x = self.embed(x)\n",
        "    u, hn = self.u_t(x, hn)\n",
        "    u = self.v_t_a(u)\n",
        "    a = self.a_t(u) # Up to equation (4) now\n",
        "    c = torch.randn(x.shape)\n",
        "    c = torch.mul(x,a)\n",
        "    c = torch.sum(c, 0)\n",
        "    c = torch.reshape(c,(1,env.customer_count,128))\n",
        "    u_bar, hu = self.bar_u_t(x,c)\n",
        "    u_bar = self.v_t_u(u_bar)\n",
        "    output = self.final(u_bar)\n",
        "    output = torch.mul(output,mask1)\n",
        "    return output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDRPWgEVNRcF"
      },
      "source": [
        "## Build the neural network for the Critic and Critic-target model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z9ifdVdNRLO"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim=4, action_dim = env.customer_count, embed_size = 128):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim, embed_size) # Perform the embedding\n",
        "    self.layer_2 = nn.Linear(embed_size, embed_size) # Adding the single dense layer\n",
        "    self.layer_3 = nn.Linear(embed_size, 1) # Adding the output layer\n",
        "\n",
        "\n",
        "  def forward(self, x, u): # x is the state, u is the action\n",
        "    # Forward-Propagation on the Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(x))\n",
        "    ws = torch.mul(x1,u)\n",
        "    x2 = F.relu(self.layer_2(ws))\n",
        "    x2 = self.layer_3(x2)\n",
        "    x2 = torch.sum(x2,1)\n",
        "    return x2\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VmkiLbHonzy"
      },
      "source": [
        "## Testing the actor and critic networks to confirm their output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ_-V52dw76O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "477249c6-6cef-43fb-df65-c83475e67ea8"
      },
      "source": [
        "env = VRPEnv()\n",
        "env.reset()\n",
        "state = env.reset()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "state = torch.Tensor(state.reshape(1,env.customer_count,4)).to(device)\n",
        "actor = Actor()\n",
        "actor_target = Actor()\n",
        "prediction = actor(state)\n",
        "prediction_target = actor_target(state)\n",
        "print(prediction)\n",
        "critic = Critic()\n",
        "q_value = critic(state,prediction)\n",
        "print(q_value)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.0000],\n",
            "         [0.0939],\n",
            "         [0.0892],\n",
            "         [0.0890],\n",
            "         [0.0868],\n",
            "         [0.0942],\n",
            "         [0.0935],\n",
            "         [0.0888],\n",
            "         [0.0950],\n",
            "         [0.0885],\n",
            "         [0.0900]]], grad_fn=<MulBackward0>)\n",
            "tensor([[-0.9976]], grad_fn=<SumBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW"
      },
      "source": [
        "## Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzd0H1xukdKe"
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Building the Training Process into a class\n",
        "class Actor_Critic(object):\n",
        "  \n",
        "  def __init__(self, state_dim):\n",
        "    self.actor = Actor(state_dim).to(device)\n",
        "    self.actor_target = Actor(state_dim).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = 0.0001)\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state, state_dim):\n",
        "    state_tensor = torch.Tensor(state.reshape(1,env.customer_count,state_dim)).to(device)\n",
        "    current_Q = actor(state_tensor).detach()\n",
        "    current_Q = current_Q.detach().numpy().reshape(env.customer_count)\n",
        "    action = np.argmax(current_Q)\n",
        "    return action, current_Q\n",
        "\n",
        "  def select_target_action(self, state):\n",
        "    state_tensor = torch.Tensor(state.reshape(1,env.customer_count,state_dim)).to(device)\n",
        "    target_Q = actor_target(state_tensor).detach()\n",
        "    target_Q = target_Q.detach().numpy().reshape(env.customer_count)\n",
        "    action = torch.argmax(target_Q)\n",
        "    return action, target_Q\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=32, discount=0.99, tau=0.005):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "      # Determine the next action based on the actor target\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Determine the target Q-value based on the critic target\n",
        "      target_Q = self.critic_target(next_state, next_action)\n",
        "\n",
        "      # Get the final target using the RL update rule\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "\n",
        "      # Determine the probabilities of the actions chosen by the actor\n",
        "      action = self.actor(state)\n",
        "\n",
        "      # Determine the current Q-value based on the critic\n",
        "      current_Q = self.critic(state,action)\n",
        "\n",
        "      # Compute the loss coming from the critic models: Critic Loss = MSE_Loss(Q, Qt)\n",
        "      critic_loss = F.mse_loss(current_Q, target_Q)\n",
        "\n",
        "      # Backpropagate this Critic loss and update the parameters of the Critic model with Adam optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward(retain_graph=True)\n",
        "      self.critic_optimizer.step()\n",
        "\n",
        "      # Update our Actor model by performing gradient descent on the output of the Critic model  \n",
        "      actor_loss = -self.critic(state, action).mean()\n",
        "      self.actor_optimizer.zero_grad()\n",
        "      actor_loss.backward()\n",
        "      self.actor_optimizer.step()\n",
        "\n",
        "      # Update the weights of the Actor target by polyak averaging\n",
        "      for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "      # Update the weights of the Critic target by polyak averaging\n",
        "      for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Make a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex"
      },
      "source": [
        "## Create a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM"
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action, current_Q = policy.select_action(obs, state_dim)\n",
        "      obs, reward, done = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U"
      },
      "source": [
        "## Set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk"
      },
      "source": [
        "env_name = \"CVRP\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "batch_size = 128 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.001 # Target network update rate"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP"
      },
      "source": [
        "## Create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af9d8eb-1c4e-47fb-fe62-7fe57ef613fc"
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"Actor_Critic\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: Actor_Critic_CVRP_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O"
      },
      "source": [
        "## Create a folder to save the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb"
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z"
      },
      "source": [
        "## Create an instance of the CVRP environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV"
      },
      "source": [
        "env = VRPEnv()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh"
      },
      "source": [
        "## Set seeds and get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj"
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.customer_count\n",
        "max_action = 1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem"
      },
      "source": [
        "## Create the policy network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6826b2ca-b0bd-4aa3-9d79-8bf874008264"
      },
      "source": [
        "print(state_dim, action_dim, max_action)\n",
        "policy = Actor_Critic(state_dim)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 11 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh"
      },
      "source": [
        "## Create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV"
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s"
      },
      "source": [
        "## Define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c174e5-5d92-4f36-f214-dd27e94fa77e"
      },
      "source": [
        "evaluations = [evaluate_policy(policy, eval_episodes=1)]\n",
        "print(env.routes)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -4.910867\n",
            "---------------------------------------\n",
            "[[0, 8, 5, 1, 6, 3, 0], [0, 10, 2, 7, 0], [0, 9, 4, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t3LV3qZaWkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33be08e6-9448-4858-eb0a-92942fd7ac43"
      },
      "source": [
        "print(env.routes)\n",
        "print(env.state)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, 8, 5, 1, 6, 3, 0], [0, 10, 2, 7, 0], [0, 9, 4, 0]]\n",
            "[[0.14920626 0.95583498 0.         2.        ]\n",
            " [0.64831157 0.02222461 0.         2.        ]\n",
            " [0.63415072 0.50040921 0.         2.        ]\n",
            " [0.42254174 0.94408989 0.         2.        ]\n",
            " [0.13045372 0.89656329 0.         2.        ]\n",
            " [0.82710122 0.48264696 0.         2.        ]\n",
            " [0.4998244  0.06814091 0.         2.        ]\n",
            " [0.38068211 0.88687239 0.         2.        ]\n",
            " [0.59348622 0.36904166 0.         2.        ]\n",
            " [0.2277177  0.64766929 0.         2.        ]\n",
            " [0.67711674 0.64106762 0.         2.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE"
      },
      "source": [
        "## Create a folder directory in which the final results will be saved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03"
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm"
      },
      "source": [
        "## Initialize the training process variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT"
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_ouY4NH_Y0I",
        "outputId": "199db34d-5086-4766-8137-222909fcf531"
      },
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "env.reset()\n",
        "total_timesteps = 0\n",
        "obs = copy.deepcopy(env.reset())\n",
        "maximum = -1000\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "    \n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {} Epsilon: {}\".format(total_timesteps, episode_num, episode_reward, epsilon))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      print(\"Total timesteps: {} Epsilon: {}\".format(total_timesteps, min(1,epsilon)))\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      if evaluations[len(evaluations)-1] > maximum:\n",
        "        policy.save(file_name, directory=\"./pytorch_models\")\n",
        "        maximum = evaluations[len(evaluations)-1]\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = copy.deepcopy(env.reset())\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Work with epsilon-greedy\n",
        "  epsilon = 50000/(total_timesteps+1)\n",
        "  np.random.seed(total_timesteps)\n",
        "  if np.random.rand() < min(1,epsilon):\n",
        "    feasible = np.array([0])\n",
        "    for i in range(1,env.customer_count):\n",
        "      if obs[i,2] < obs[i,3] and obs[i,2] != 0:\n",
        "        feasible = np.concatenate((feasible,np.array([i])))\n",
        "    if len(feasible) > 1:\n",
        "      feasible = np.delete(feasible,0)\n",
        "    action = np.random.choice(feasible)\n",
        "  else: # Choose greedy action\n",
        "    action, current_Q = policy.select_action(obs, state_dim)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done = env.step(action)\n",
        "\n",
        "  # Check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "\n",
        "  # Increase the total reward\n",
        "  episode_reward += reward\n",
        "\n",
        "  # Store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, copy.deepcopy(new_obs), action, reward, done_bool)) # Have to use deepcopy to prevent overwriting of historic next states\n",
        "\n",
        "  # Update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = copy.deepcopy(new_obs)\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# Add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 11 Episode Num: 1 Reward: -6.415880080576345 Epsilon: 4545.454545454545\n",
            "Total Timesteps: 22 Episode Num: 2 Reward: -7.249635843630094 Epsilon: 2272.7272727272725\n",
            "Total Timesteps: 33 Episode Num: 3 Reward: -6.038779131421817 Epsilon: 1515.1515151515152\n",
            "Total Timesteps: 44 Episode Num: 4 Reward: -5.16669365049552 Epsilon: 1136.3636363636363\n",
            "Total Timesteps: 55 Episode Num: 5 Reward: -4.338452739822076 Epsilon: 909.0909090909091\n",
            "Total Timesteps: 66 Episode Num: 6 Reward: -5.696506431367579 Epsilon: 757.5757575757576\n",
            "Total Timesteps: 77 Episode Num: 7 Reward: -5.913708560452239 Epsilon: 649.3506493506494\n",
            "Total Timesteps: 88 Episode Num: 8 Reward: -4.98217641193541 Epsilon: 568.1818181818181\n",
            "Total Timesteps: 99 Episode Num: 9 Reward: -4.222883766408776 Epsilon: 505.050505050505\n",
            "Total Timesteps: 110 Episode Num: 10 Reward: -4.946556349380542 Epsilon: 454.54545454545456\n",
            "Total Timesteps: 121 Episode Num: 11 Reward: -5.197268324363192 Epsilon: 413.22314049586777\n",
            "Total Timesteps: 132 Episode Num: 12 Reward: -5.705866555233828 Epsilon: 378.7878787878788\n",
            "Total Timesteps: 143 Episode Num: 13 Reward: -6.704514870005835 Epsilon: 349.65034965034965\n",
            "Total Timesteps: 154 Episode Num: 14 Reward: -6.898161204154958 Epsilon: 324.6753246753247\n",
            "Total Timesteps: 165 Episode Num: 15 Reward: -7.2690947587950525 Epsilon: 303.030303030303\n",
            "Total Timesteps: 176 Episode Num: 16 Reward: -6.437604325386473 Epsilon: 284.09090909090907\n",
            "Total Timesteps: 187 Episode Num: 17 Reward: -4.594195177548171 Epsilon: 267.379679144385\n",
            "Total Timesteps: 198 Episode Num: 18 Reward: -5.09357840311433 Epsilon: 252.5252525252525\n",
            "Total Timesteps: 209 Episode Num: 19 Reward: -4.302762435413082 Epsilon: 239.23444976076556\n",
            "Total Timesteps: 220 Episode Num: 20 Reward: -5.074217664087448 Epsilon: 227.27272727272728\n",
            "Total Timesteps: 231 Episode Num: 21 Reward: -5.7502446845489805 Epsilon: 216.45021645021646\n",
            "Total Timesteps: 242 Episode Num: 22 Reward: -6.391614500020192 Epsilon: 206.61157024793388\n",
            "Total Timesteps: 253 Episode Num: 23 Reward: -6.403510054467345 Epsilon: 197.62845849802372\n",
            "Total Timesteps: 264 Episode Num: 24 Reward: -4.083439080376912 Epsilon: 189.3939393939394\n",
            "Total Timesteps: 275 Episode Num: 25 Reward: -5.993762613781257 Epsilon: 181.8181818181818\n",
            "Total Timesteps: 286 Episode Num: 26 Reward: -5.099285477181645 Epsilon: 174.82517482517483\n",
            "Total Timesteps: 297 Episode Num: 27 Reward: -6.30768507506614 Epsilon: 168.35016835016836\n",
            "Total Timesteps: 308 Episode Num: 28 Reward: -6.408868471823924 Epsilon: 162.33766233766235\n",
            "Total Timesteps: 319 Episode Num: 29 Reward: -6.387746065041952 Epsilon: 156.73981191222572\n",
            "Total Timesteps: 330 Episode Num: 30 Reward: -4.090452754609654 Epsilon: 151.5151515151515\n",
            "Total Timesteps: 341 Episode Num: 31 Reward: -6.752986804550183 Epsilon: 146.6275659824047\n",
            "Total Timesteps: 352 Episode Num: 32 Reward: -6.627382851941243 Epsilon: 142.04545454545453\n",
            "Total Timesteps: 363 Episode Num: 33 Reward: -6.007764071292694 Epsilon: 137.7410468319559\n",
            "Total Timesteps: 374 Episode Num: 34 Reward: -5.376567179187901 Epsilon: 133.6898395721925\n",
            "Total Timesteps: 385 Episode Num: 35 Reward: -6.797846723418059 Epsilon: 129.87012987012986\n",
            "Total Timesteps: 396 Episode Num: 36 Reward: -5.246542677488765 Epsilon: 126.26262626262626\n",
            "Total Timesteps: 407 Episode Num: 37 Reward: -6.1946531389040285 Epsilon: 122.85012285012284\n",
            "Total Timesteps: 418 Episode Num: 38 Reward: -6.826105135953413 Epsilon: 119.61722488038278\n",
            "Total Timesteps: 429 Episode Num: 39 Reward: -6.995981965252494 Epsilon: 116.55011655011656\n",
            "Total Timesteps: 440 Episode Num: 40 Reward: -7.699186907619418 Epsilon: 113.63636363636364\n",
            "Total Timesteps: 451 Episode Num: 41 Reward: -5.341750446818412 Epsilon: 110.86474501108647\n",
            "Total Timesteps: 462 Episode Num: 42 Reward: -4.9205835435478935 Epsilon: 108.22510822510823\n",
            "Total Timesteps: 473 Episode Num: 43 Reward: -6.7966703276699185 Epsilon: 105.70824524312897\n",
            "Total Timesteps: 484 Episode Num: 44 Reward: -5.401317243083338 Epsilon: 103.30578512396694\n",
            "Total Timesteps: 495 Episode Num: 45 Reward: -5.3928049382869165 Epsilon: 101.01010101010101\n",
            "Total Timesteps: 506 Episode Num: 46 Reward: -6.3426151084031055 Epsilon: 98.81422924901186\n",
            "Total Timesteps: 517 Episode Num: 47 Reward: -5.418103146877091 Epsilon: 96.71179883945841\n",
            "Total Timesteps: 528 Episode Num: 48 Reward: -3.639675949859757 Epsilon: 94.6969696969697\n",
            "Total Timesteps: 539 Episode Num: 49 Reward: -5.193992082854603 Epsilon: 92.7643784786642\n",
            "Total Timesteps: 550 Episode Num: 50 Reward: -5.497538237315618 Epsilon: 90.9090909090909\n",
            "Total Timesteps: 561 Episode Num: 51 Reward: -6.614973994078682 Epsilon: 89.12655971479501\n",
            "Total Timesteps: 572 Episode Num: 52 Reward: -4.078652478193541 Epsilon: 87.41258741258741\n",
            "Total Timesteps: 583 Episode Num: 53 Reward: -6.356535225613579 Epsilon: 85.76329331046313\n",
            "Total Timesteps: 594 Episode Num: 54 Reward: -5.124616452805236 Epsilon: 84.17508417508418\n",
            "Total Timesteps: 605 Episode Num: 55 Reward: -6.542867222912653 Epsilon: 82.64462809917356\n",
            "Total Timesteps: 616 Episode Num: 56 Reward: -5.0439891848634195 Epsilon: 81.16883116883118\n",
            "Total Timesteps: 627 Episode Num: 57 Reward: -5.6898131961765905 Epsilon: 79.74481658692186\n",
            "Total Timesteps: 638 Episode Num: 58 Reward: -4.821551948798089 Epsilon: 78.36990595611286\n",
            "Total Timesteps: 649 Episode Num: 59 Reward: -4.972950925332624 Epsilon: 77.04160246533128\n",
            "Total Timesteps: 660 Episode Num: 60 Reward: -6.030097558536406 Epsilon: 75.75757575757575\n",
            "Total Timesteps: 671 Episode Num: 61 Reward: -5.3239768933993625 Epsilon: 74.51564828614009\n",
            "Total Timesteps: 682 Episode Num: 62 Reward: -4.924717350100747 Epsilon: 73.31378299120234\n",
            "Total Timesteps: 693 Episode Num: 63 Reward: -6.8078653811992 Epsilon: 72.15007215007215\n",
            "Total Timesteps: 704 Episode Num: 64 Reward: -5.724669177092819 Epsilon: 71.02272727272727\n",
            "Total Timesteps: 715 Episode Num: 65 Reward: -3.9520765820297385 Epsilon: 69.93006993006993\n",
            "Total Timesteps: 726 Episode Num: 66 Reward: -4.810990141644289 Epsilon: 68.87052341597796\n",
            "Total Timesteps: 737 Episode Num: 67 Reward: -6.604295053648997 Epsilon: 67.84260515603799\n",
            "Total Timesteps: 748 Episode Num: 68 Reward: -7.782250152006791 Epsilon: 66.84491978609626\n",
            "Total Timesteps: 759 Episode Num: 69 Reward: -5.88156389057557 Epsilon: 65.87615283267458\n",
            "Total Timesteps: 770 Episode Num: 70 Reward: -6.858685581856937 Epsilon: 64.93506493506493\n",
            "Total Timesteps: 781 Episode Num: 71 Reward: -6.602483042986222 Epsilon: 64.02048655569783\n",
            "Total Timesteps: 792 Episode Num: 72 Reward: -6.654223762971713 Epsilon: 63.13131313131313\n",
            "Total Timesteps: 803 Episode Num: 73 Reward: -6.2427946477899905 Epsilon: 62.266500622665006\n",
            "Total Timesteps: 814 Episode Num: 74 Reward: -6.223642546923357 Epsilon: 61.42506142506142\n",
            "Total Timesteps: 825 Episode Num: 75 Reward: -6.232283762495235 Epsilon: 60.60606060606061\n",
            "Total Timesteps: 836 Episode Num: 76 Reward: -5.555112321507096 Epsilon: 59.80861244019139\n",
            "Total Timesteps: 847 Episode Num: 77 Reward: -5.75932274103161 Epsilon: 59.031877213695395\n",
            "Total Timesteps: 858 Episode Num: 78 Reward: -6.396069783200936 Epsilon: 58.27505827505828\n",
            "Total Timesteps: 869 Episode Num: 79 Reward: -5.137388765263611 Epsilon: 57.537399309551205\n",
            "Total Timesteps: 880 Episode Num: 80 Reward: -4.89615533744807 Epsilon: 56.81818181818182\n",
            "Total Timesteps: 891 Episode Num: 81 Reward: -5.702389609311586 Epsilon: 56.11672278338945\n",
            "Total Timesteps: 902 Episode Num: 82 Reward: -5.0118160461585655 Epsilon: 55.432372505543235\n",
            "Total Timesteps: 913 Episode Num: 83 Reward: -7.555577967327583 Epsilon: 54.7645125958379\n",
            "Total Timesteps: 924 Episode Num: 84 Reward: -7.475693632724265 Epsilon: 54.112554112554115\n",
            "Total Timesteps: 935 Episode Num: 85 Reward: -5.343131624161369 Epsilon: 53.475935828877006\n",
            "Total Timesteps: 946 Episode Num: 86 Reward: -6.467534783811641 Epsilon: 52.854122621564485\n",
            "Total Timesteps: 957 Episode Num: 87 Reward: -6.057706047880451 Epsilon: 52.2466039707419\n",
            "Total Timesteps: 968 Episode Num: 88 Reward: -6.304213822336653 Epsilon: 51.65289256198347\n",
            "Total Timesteps: 979 Episode Num: 89 Reward: -4.802313833268493 Epsilon: 51.07252298263534\n",
            "Total Timesteps: 990 Episode Num: 90 Reward: -4.354984895142381 Epsilon: 50.505050505050505\n",
            "Total Timesteps: 1001 Episode Num: 91 Reward: -5.973354742754196 Epsilon: 49.95004995004995\n",
            "Total Timesteps: 1012 Episode Num: 92 Reward: -5.9504169352696294 Epsilon: 49.40711462450593\n",
            "Total Timesteps: 1023 Episode Num: 93 Reward: -6.468286416347931 Epsilon: 48.87585532746823\n",
            "Total Timesteps: 1034 Episode Num: 94 Reward: -6.014693975131167 Epsilon: 48.355899419729205\n",
            "Total Timesteps: 1045 Episode Num: 95 Reward: -6.429908042601624 Epsilon: 47.84688995215311\n",
            "Total Timesteps: 1056 Episode Num: 96 Reward: -5.593118994917617 Epsilon: 47.34848484848485\n",
            "Total Timesteps: 1067 Episode Num: 97 Reward: -3.8036658373033188 Epsilon: 46.860356138706656\n",
            "Total Timesteps: 1078 Episode Num: 98 Reward: -5.1618047808326395 Epsilon: 46.3821892393321\n",
            "Total Timesteps: 1089 Episode Num: 99 Reward: -3.7768009514660834 Epsilon: 45.91368227731864\n",
            "Total Timesteps: 1100 Episode Num: 100 Reward: -4.874765293989031 Epsilon: 45.45454545454545\n",
            "Total Timesteps: 1111 Episode Num: 101 Reward: -7.271608793455153 Epsilon: 45.00450045004501\n",
            "Total Timesteps: 1122 Episode Num: 102 Reward: -8.319501668675878 Epsilon: 44.563279857397504\n",
            "Total Timesteps: 1133 Episode Num: 103 Reward: -6.2553325300402935 Epsilon: 44.1306266548985\n",
            "Total Timesteps: 1144 Episode Num: 104 Reward: -5.539610030369123 Epsilon: 43.70629370629371\n",
            "Total Timesteps: 1155 Episode Num: 105 Reward: -5.841334508417589 Epsilon: 43.29004329004329\n",
            "Total Timesteps: 1166 Episode Num: 106 Reward: -5.828939222096398 Epsilon: 42.88164665523156\n",
            "Total Timesteps: 1177 Episode Num: 107 Reward: -4.921041589098369 Epsilon: 42.48088360237893\n",
            "Total Timesteps: 1188 Episode Num: 108 Reward: -4.251801951775036 Epsilon: 42.08754208754209\n",
            "Total Timesteps: 1199 Episode Num: 109 Reward: -5.209664632359194 Epsilon: 41.70141784820684\n",
            "Total Timesteps: 1210 Episode Num: 110 Reward: -5.586088260735318 Epsilon: 41.32231404958678\n",
            "Total Timesteps: 1221 Episode Num: 111 Reward: -6.519361624748342 Epsilon: 40.95004095004095\n",
            "Total Timesteps: 1232 Episode Num: 112 Reward: -4.801472262249731 Epsilon: 40.58441558441559\n",
            "Total Timesteps: 1243 Episode Num: 113 Reward: -4.474078652154042 Epsilon: 40.22526146419952\n",
            "Total Timesteps: 1254 Episode Num: 114 Reward: -5.248837883296607 Epsilon: 39.87240829346093\n",
            "Total Timesteps: 1265 Episode Num: 115 Reward: -5.738204732389221 Epsilon: 39.52569169960474\n",
            "Total Timesteps: 1276 Episode Num: 116 Reward: -6.022673477697413 Epsilon: 39.18495297805643\n",
            "Total Timesteps: 1287 Episode Num: 117 Reward: -6.6408468221150265 Epsilon: 38.85003885003885\n",
            "Total Timesteps: 1298 Episode Num: 118 Reward: -5.546672056816909 Epsilon: 38.52080123266564\n",
            "Total Timesteps: 1309 Episode Num: 119 Reward: -6.496097787284197 Epsilon: 38.19709702062643\n",
            "Total Timesteps: 1320 Episode Num: 120 Reward: -4.796020119746115 Epsilon: 37.878787878787875\n",
            "Total Timesteps: 1331 Episode Num: 121 Reward: -6.446616997416181 Epsilon: 37.56574004507889\n",
            "Total Timesteps: 1342 Episode Num: 122 Reward: -4.597361406593006 Epsilon: 37.257824143070046\n",
            "Total Timesteps: 1353 Episode Num: 123 Reward: -4.492317786576497 Epsilon: 36.95491500369549\n",
            "Total Timesteps: 1364 Episode Num: 124 Reward: -6.655290238493323 Epsilon: 36.65689149560117\n",
            "Total Timesteps: 1375 Episode Num: 125 Reward: -5.854748602793267 Epsilon: 36.36363636363637\n",
            "Total Timesteps: 1386 Episode Num: 126 Reward: -6.321539490187762 Epsilon: 36.075036075036074\n",
            "Total Timesteps: 1397 Episode Num: 127 Reward: -7.085702482424341 Epsilon: 35.79098067287044\n",
            "Total Timesteps: 1408 Episode Num: 128 Reward: -4.172414163753017 Epsilon: 35.51136363636363\n",
            "Total Timesteps: 1419 Episode Num: 129 Reward: -5.723050703616952 Epsilon: 35.236081747709655\n",
            "Total Timesteps: 1430 Episode Num: 130 Reward: -6.090058485765955 Epsilon: 34.96503496503497\n",
            "Total Timesteps: 1441 Episode Num: 131 Reward: -4.810011826056759 Epsilon: 34.698126301179734\n",
            "Total Timesteps: 1452 Episode Num: 132 Reward: -7.2596944272926285 Epsilon: 34.43526170798898\n",
            "Total Timesteps: 1463 Episode Num: 133 Reward: -4.845680905659197 Epsilon: 34.17634996582365\n",
            "Total Timesteps: 1474 Episode Num: 134 Reward: -6.051052193276898 Epsilon: 33.921302578018995\n",
            "Total Timesteps: 1485 Episode Num: 135 Reward: -5.133472620469544 Epsilon: 33.67003367003367\n",
            "Total Timesteps: 1496 Episode Num: 136 Reward: -5.51903310557381 Epsilon: 33.42245989304813\n",
            "Total Timesteps: 1507 Episode Num: 137 Reward: -5.165633397672862 Epsilon: 33.178500331785\n",
            "Total Timesteps: 1518 Episode Num: 138 Reward: -6.971237040158774 Epsilon: 32.93807641633729\n",
            "Total Timesteps: 1529 Episode Num: 139 Reward: -5.636426691408636 Epsilon: 32.701111837802486\n",
            "Total Timesteps: 1540 Episode Num: 140 Reward: -4.417797544727111 Epsilon: 32.467532467532465\n",
            "Total Timesteps: 1551 Episode Num: 141 Reward: -4.454749781783471 Epsilon: 32.23726627981947\n",
            "Total Timesteps: 1562 Episode Num: 142 Reward: -5.957698620116823 Epsilon: 32.010243277848915\n",
            "Total Timesteps: 1573 Episode Num: 143 Reward: -5.5183487392512 Epsilon: 31.78639542275906\n",
            "Total Timesteps: 1584 Episode Num: 144 Reward: -4.993654272002302 Epsilon: 31.565656565656564\n",
            "Total Timesteps: 1595 Episode Num: 145 Reward: -6.09406872706794 Epsilon: 31.34796238244514\n",
            "Total Timesteps: 1606 Episode Num: 146 Reward: -5.293015704292662 Epsilon: 31.133250311332503\n",
            "Total Timesteps: 1617 Episode Num: 147 Reward: -6.092167471743055 Epsilon: 30.921459492888065\n",
            "Total Timesteps: 1628 Episode Num: 148 Reward: -5.408311577992822 Epsilon: 30.71253071253071\n",
            "Total Timesteps: 1639 Episode Num: 149 Reward: -5.211863118381695 Epsilon: 30.50640634533252\n",
            "Total Timesteps: 1650 Episode Num: 150 Reward: -6.193570595961113 Epsilon: 30.303030303030305\n",
            "Total Timesteps: 1661 Episode Num: 151 Reward: -5.4905052288108225 Epsilon: 30.102347983142685\n",
            "Total Timesteps: 1672 Episode Num: 152 Reward: -5.212317851248281 Epsilon: 29.904306220095695\n",
            "Total Timesteps: 1683 Episode Num: 153 Reward: -5.753246074577165 Epsilon: 29.708853238265004\n",
            "Total Timesteps: 1694 Episode Num: 154 Reward: -5.4263749611693335 Epsilon: 29.515938606847698\n",
            "Total Timesteps: 1705 Episode Num: 155 Reward: -6.527647642389085 Epsilon: 29.325513196480937\n",
            "Total Timesteps: 1716 Episode Num: 156 Reward: -5.654095082851355 Epsilon: 29.13752913752914\n",
            "Total Timesteps: 1727 Episode Num: 157 Reward: -5.828755312413033 Epsilon: 28.951939779965258\n",
            "Total Timesteps: 1738 Episode Num: 158 Reward: -6.261575363754619 Epsilon: 28.768699654775602\n",
            "Total Timesteps: 1749 Episode Num: 159 Reward: -6.5052743516544265 Epsilon: 28.58776443682104\n",
            "Total Timesteps: 1760 Episode Num: 160 Reward: -4.461308952681057 Epsilon: 28.40909090909091\n",
            "Total Timesteps: 1771 Episode Num: 161 Reward: -6.090589907740479 Epsilon: 28.232636928289104\n",
            "Total Timesteps: 1782 Episode Num: 162 Reward: -3.9300402221062525 Epsilon: 28.058361391694724\n",
            "Total Timesteps: 1793 Episode Num: 163 Reward: -5.658419519105538 Epsilon: 27.88622420524261\n",
            "Total Timesteps: 1804 Episode Num: 164 Reward: -5.86642951518286 Epsilon: 27.716186252771617\n",
            "Total Timesteps: 1815 Episode Num: 165 Reward: -5.7825396329999235 Epsilon: 27.548209366391184\n",
            "Total Timesteps: 1826 Episode Num: 166 Reward: -5.049388535587674 Epsilon: 27.38225629791895\n",
            "Total Timesteps: 1837 Episode Num: 167 Reward: -6.728339760810549 Epsilon: 27.218290691344585\n",
            "Total Timesteps: 1848 Episode Num: 168 Reward: -5.101012496203239 Epsilon: 27.056277056277057\n",
            "Total Timesteps: 1859 Episode Num: 169 Reward: -5.7125724289179285 Epsilon: 26.89618074233459\n",
            "Total Timesteps: 1870 Episode Num: 170 Reward: -4.420411251876281 Epsilon: 26.737967914438503\n",
            "Total Timesteps: 1881 Episode Num: 171 Reward: -5.261375123469387 Epsilon: 26.58160552897395\n",
            "Total Timesteps: 1892 Episode Num: 172 Reward: -4.929372765730086 Epsilon: 26.427061310782243\n",
            "Total Timesteps: 1903 Episode Num: 173 Reward: -4.01948117521447 Epsilon: 26.27430373095113\n",
            "Total Timesteps: 1914 Episode Num: 174 Reward: -7.869247727926332 Epsilon: 26.12330198537095\n",
            "Total Timesteps: 1925 Episode Num: 175 Reward: -4.682484522055718 Epsilon: 25.974025974025974\n",
            "Total Timesteps: 1936 Episode Num: 176 Reward: -5.7070220440693085 Epsilon: 25.826446280991735\n",
            "Total Timesteps: 1947 Episode Num: 177 Reward: -6.767205985536535 Epsilon: 25.680534155110426\n",
            "Total Timesteps: 1958 Episode Num: 178 Reward: -5.287060712243816 Epsilon: 25.53626149131767\n",
            "Total Timesteps: 1969 Episode Num: 179 Reward: -7.3755500686827755 Epsilon: 25.393600812595228\n",
            "Total Timesteps: 1980 Episode Num: 180 Reward: -4.779840590373102 Epsilon: 25.252525252525253\n",
            "Total Timesteps: 1991 Episode Num: 181 Reward: -5.326982738830148 Epsilon: 25.113008538422903\n",
            "Total Timesteps: 2002 Episode Num: 182 Reward: -5.319598504129037 Epsilon: 24.975024975024976\n",
            "Total Timesteps: 2013 Episode Num: 183 Reward: -5.219430126258627 Epsilon: 24.838549428713364\n",
            "Total Timesteps: 2024 Episode Num: 184 Reward: -4.767078309647415 Epsilon: 24.703557312252965\n",
            "Total Timesteps: 2035 Episode Num: 185 Reward: -6.5452925523279895 Epsilon: 24.57002457002457\n",
            "Total Timesteps: 2046 Episode Num: 186 Reward: -4.987706428615321 Epsilon: 24.437927663734115\n",
            "Total Timesteps: 2057 Episode Num: 187 Reward: -6.563168142396631 Epsilon: 24.307243558580456\n",
            "Total Timesteps: 2068 Episode Num: 188 Reward: -4.935210101678546 Epsilon: 24.177949709864603\n",
            "Total Timesteps: 2079 Episode Num: 189 Reward: -5.92054579594545 Epsilon: 24.05002405002405\n",
            "Total Timesteps: 2090 Episode Num: 190 Reward: -5.018394930735939 Epsilon: 23.923444976076556\n",
            "Total Timesteps: 2101 Episode Num: 191 Reward: -6.825389270163187 Epsilon: 23.798191337458352\n",
            "Total Timesteps: 2112 Episode Num: 192 Reward: -4.0075781459679725 Epsilon: 23.674242424242426\n",
            "Total Timesteps: 2123 Episode Num: 193 Reward: -6.3650072330714 Epsilon: 23.551577955723033\n",
            "Total Timesteps: 2134 Episode Num: 194 Reward: -6.514311735626355 Epsilon: 23.430178069353328\n",
            "Total Timesteps: 2145 Episode Num: 195 Reward: -7.337793211866408 Epsilon: 23.31002331002331\n",
            "Total Timesteps: 2156 Episode Num: 196 Reward: -4.417915483660571 Epsilon: 23.19109461966605\n",
            "Total Timesteps: 2167 Episode Num: 197 Reward: -5.6693441015733 Epsilon: 23.073373327180434\n",
            "Total Timesteps: 2178 Episode Num: 198 Reward: -6.266113849787067 Epsilon: 22.95684113865932\n",
            "Total Timesteps: 2189 Episode Num: 199 Reward: -6.3020895479142975 Epsilon: 22.841480127912288\n",
            "Total Timesteps: 2200 Episode Num: 200 Reward: -5.893906408968915 Epsilon: 22.727272727272727\n",
            "Total Timesteps: 2211 Episode Num: 201 Reward: -6.9275554115363605 Epsilon: 22.61420171867933\n",
            "Total Timesteps: 2222 Episode Num: 202 Reward: -6.828948736987716 Epsilon: 22.502250225022504\n",
            "Total Timesteps: 2233 Episode Num: 203 Reward: -4.951976996671486 Epsilon: 22.391401701746528\n",
            "Total Timesteps: 2244 Episode Num: 204 Reward: -5.4870853434993085 Epsilon: 22.281639928698752\n",
            "Total Timesteps: 2255 Episode Num: 205 Reward: -6.508318089820377 Epsilon: 22.172949002217294\n",
            "Total Timesteps: 2266 Episode Num: 206 Reward: -7.609870982877797 Epsilon: 22.06531332744925\n",
            "Total Timesteps: 2277 Episode Num: 207 Reward: -5.099564065309938 Epsilon: 21.958717610891522\n",
            "Total Timesteps: 2288 Episode Num: 208 Reward: -6.444263650010103 Epsilon: 21.853146853146853\n",
            "Total Timesteps: 2299 Episode Num: 209 Reward: -5.64376358928402 Epsilon: 21.748586341887776\n",
            "Total Timesteps: 2310 Episode Num: 210 Reward: -4.149384495535921 Epsilon: 21.645021645021647\n",
            "Total Timesteps: 2321 Episode Num: 211 Reward: -5.052213530791847 Epsilon: 21.542438604049977\n",
            "Total Timesteps: 2332 Episode Num: 212 Reward: -5.217010806190089 Epsilon: 21.44082332761578\n",
            "Total Timesteps: 2343 Episode Num: 213 Reward: -6.231932613656767 Epsilon: 21.340162185232607\n",
            "Total Timesteps: 2354 Episode Num: 214 Reward: -6.61172825287442 Epsilon: 21.240441801189466\n",
            "Total Timesteps: 2365 Episode Num: 215 Reward: -4.465974457087439 Epsilon: 21.141649048625794\n",
            "Total Timesteps: 2376 Episode Num: 216 Reward: -5.543877588389098 Epsilon: 21.043771043771045\n",
            "Total Timesteps: 2387 Episode Num: 217 Reward: -7.8116688563579 Epsilon: 20.94679514034353\n",
            "Total Timesteps: 2398 Episode Num: 218 Reward: -6.196349974118962 Epsilon: 20.85070892410342\n",
            "Total Timesteps: 2409 Episode Num: 219 Reward: -4.652254517178824 Epsilon: 20.755500207555002\n",
            "Total Timesteps: 2420 Episode Num: 220 Reward: -5.777203030823668 Epsilon: 20.66115702479339\n",
            "Total Timesteps: 2431 Episode Num: 221 Reward: -5.424292881183147 Epsilon: 20.567667626491154\n",
            "Total Timesteps: 2442 Episode Num: 222 Reward: -4.3797257233569145 Epsilon: 20.475020475020475\n",
            "Total Timesteps: 2453 Episode Num: 223 Reward: -5.276340458627389 Epsilon: 20.383204239706483\n",
            "Total Timesteps: 2464 Episode Num: 224 Reward: -5.357500059590135 Epsilon: 20.292207792207794\n",
            "Total Timesteps: 2475 Episode Num: 225 Reward: -4.430036962700971 Epsilon: 20.2020202020202\n",
            "Total Timesteps: 2486 Episode Num: 226 Reward: -6.215298943605239 Epsilon: 20.11263073209976\n",
            "Total Timesteps: 2497 Episode Num: 227 Reward: -5.8552931233389796 Epsilon: 20.024028834601523\n",
            "Total Timesteps: 2508 Episode Num: 228 Reward: -5.319528184915494 Epsilon: 19.936204146730464\n",
            "Total Timesteps: 2519 Episode Num: 229 Reward: -5.378342287634756 Epsilon: 19.849146486701073\n",
            "Total Timesteps: 2530 Episode Num: 230 Reward: -7.701890453569135 Epsilon: 19.76284584980237\n",
            "Total Timesteps: 2541 Episode Num: 231 Reward: -6.478413724177392 Epsilon: 19.677292404565133\n",
            "Total Timesteps: 2552 Episode Num: 232 Reward: -6.010415987445876 Epsilon: 19.592476489028215\n",
            "Total Timesteps: 2563 Episode Num: 233 Reward: -6.140931417577884 Epsilon: 19.508388607101054\n",
            "Total Timesteps: 2574 Episode Num: 234 Reward: -6.764044957867108 Epsilon: 19.425019425019425\n",
            "Total Timesteps: 2585 Episode Num: 235 Reward: -5.968964510747179 Epsilon: 19.342359767891683\n",
            "Total Timesteps: 2596 Episode Num: 236 Reward: -5.016369630030713 Epsilon: 19.26040061633282\n",
            "Total Timesteps: 2607 Episode Num: 237 Reward: -5.502238330354674 Epsilon: 19.179133103183737\n",
            "Total Timesteps: 2618 Episode Num: 238 Reward: -6.21984679639484 Epsilon: 19.098548510313215\n",
            "Total Timesteps: 2629 Episode Num: 239 Reward: -4.998262051192776 Epsilon: 19.01863826550019\n",
            "Total Timesteps: 2640 Episode Num: 240 Reward: -6.497006390327747 Epsilon: 18.939393939393938\n",
            "Total Timesteps: 2651 Episode Num: 241 Reward: -5.279943406407048 Epsilon: 18.86080724254998\n",
            "Total Timesteps: 2662 Episode Num: 242 Reward: -5.142275147695398 Epsilon: 18.782870022539445\n",
            "Total Timesteps: 2673 Episode Num: 243 Reward: -6.008749228606861 Epsilon: 18.705574261129815\n",
            "Total Timesteps: 2684 Episode Num: 244 Reward: -6.671806497160425 Epsilon: 18.628912071535023\n",
            "Total Timesteps: 2695 Episode Num: 245 Reward: -5.466762471781418 Epsilon: 18.552875695732837\n",
            "Total Timesteps: 2706 Episode Num: 246 Reward: -5.2706158589788705 Epsilon: 18.477457501847745\n",
            "Total Timesteps: 2717 Episode Num: 247 Reward: -5.59181271081181 Epsilon: 18.40264998159735\n",
            "Total Timesteps: 2728 Episode Num: 248 Reward: -5.2594752156401405 Epsilon: 18.328445747800586\n",
            "Total Timesteps: 2739 Episode Num: 249 Reward: -4.720672517117627 Epsilon: 18.254837531945967\n",
            "Total Timesteps: 2750 Episode Num: 250 Reward: -4.872681734135016 Epsilon: 18.181818181818183\n",
            "Total Timesteps: 2761 Episode Num: 251 Reward: -6.247405628363462 Epsilon: 18.109380659181458\n",
            "Total Timesteps: 2772 Episode Num: 252 Reward: -5.469450365936249 Epsilon: 18.037518037518037\n",
            "Total Timesteps: 2783 Episode Num: 253 Reward: -5.411065862327176 Epsilon: 17.966223499820337\n",
            "Total Timesteps: 2794 Episode Num: 254 Reward: -4.517764205940243 Epsilon: 17.89549033643522\n",
            "Total Timesteps: 2805 Episode Num: 255 Reward: -7.017231272866196 Epsilon: 17.825311942959\n",
            "Total Timesteps: 2816 Episode Num: 256 Reward: -6.432331281147586 Epsilon: 17.755681818181817\n",
            "Total Timesteps: 2827 Episode Num: 257 Reward: -4.7501399916369005 Epsilon: 17.686593562079942\n",
            "Total Timesteps: 2838 Episode Num: 258 Reward: -5.368670914063039 Epsilon: 17.618040873854827\n",
            "Total Timesteps: 2849 Episode Num: 259 Reward: -5.644137340652608 Epsilon: 17.55001755001755\n",
            "Total Timesteps: 2860 Episode Num: 260 Reward: -7.051773639922019 Epsilon: 17.482517482517483\n",
            "Total Timesteps: 2871 Episode Num: 261 Reward: -6.0003730168547085 Epsilon: 17.415534656913966\n",
            "Total Timesteps: 2882 Episode Num: 262 Reward: -5.801953697353355 Epsilon: 17.349063150589867\n",
            "Total Timesteps: 2893 Episode Num: 263 Reward: -6.2098367395715846 Epsilon: 17.283097131005878\n",
            "Total Timesteps: 2904 Episode Num: 264 Reward: -5.241268543094277 Epsilon: 17.21763085399449\n",
            "Total Timesteps: 2915 Episode Num: 265 Reward: -3.5773905965018975 Epsilon: 17.152658662092623\n",
            "Total Timesteps: 2926 Episode Num: 266 Reward: -8.484745866259306 Epsilon: 17.088174982911823\n",
            "Total Timesteps: 2937 Episode Num: 267 Reward: -5.541844479127813 Epsilon: 17.024174327545115\n",
            "Total Timesteps: 2948 Episode Num: 268 Reward: -4.93117347008656 Epsilon: 16.960651289009498\n",
            "Total Timesteps: 2959 Episode Num: 269 Reward: -5.685123044739586 Epsilon: 16.897600540723218\n",
            "Total Timesteps: 2970 Episode Num: 270 Reward: -5.413708311535716 Epsilon: 16.835016835016834\n",
            "Total Timesteps: 2981 Episode Num: 271 Reward: -6.365227968274535 Epsilon: 16.772895001677288\n",
            "Total Timesteps: 2992 Episode Num: 272 Reward: -6.165493520449305 Epsilon: 16.711229946524064\n",
            "Total Timesteps: 3003 Episode Num: 273 Reward: -5.577272675787084 Epsilon: 16.65001665001665\n",
            "Total Timesteps: 3014 Episode Num: 274 Reward: -5.581645316622973 Epsilon: 16.5892501658925\n",
            "Total Timesteps: 3025 Episode Num: 275 Reward: -5.909178478779951 Epsilon: 16.52892561983471\n",
            "Total Timesteps: 3036 Episode Num: 276 Reward: -5.783059775322645 Epsilon: 16.469038208168644\n",
            "Total Timesteps: 3047 Episode Num: 277 Reward: -4.408016566270623 Epsilon: 16.409583196586805\n",
            "Total Timesteps: 3058 Episode Num: 278 Reward: -6.164310709405367 Epsilon: 16.350555918901243\n",
            "Total Timesteps: 3069 Episode Num: 279 Reward: -4.680400273708133 Epsilon: 16.291951775822742\n",
            "Total Timesteps: 3080 Episode Num: 280 Reward: -6.603522238529399 Epsilon: 16.233766233766232\n",
            "Total Timesteps: 3091 Episode Num: 281 Reward: -5.794927288410429 Epsilon: 16.175994823681656\n",
            "Total Timesteps: 3102 Episode Num: 282 Reward: -6.286742537750552 Epsilon: 16.118633139909736\n",
            "Total Timesteps: 3113 Episode Num: 283 Reward: -5.641033534152237 Epsilon: 16.061676839062\n",
            "Total Timesteps: 3124 Episode Num: 284 Reward: -7.029547884391 Epsilon: 16.005121638924457\n",
            "Total Timesteps: 3135 Episode Num: 285 Reward: -4.591899220900068 Epsilon: 15.94896331738437\n",
            "Total Timesteps: 3146 Episode Num: 286 Reward: -5.4306481905041695 Epsilon: 15.89319771137953\n",
            "Total Timesteps: 3157 Episode Num: 287 Reward: -4.573221592124406 Epsilon: 15.837820715869496\n",
            "Total Timesteps: 3168 Episode Num: 288 Reward: -4.659727658725652 Epsilon: 15.782828282828282\n",
            "Total Timesteps: 3179 Episode Num: 289 Reward: -4.845381812293805 Epsilon: 15.728216420257942\n",
            "Total Timesteps: 3190 Episode Num: 290 Reward: -4.731860251199952 Epsilon: 15.67398119122257\n",
            "Total Timesteps: 3201 Episode Num: 291 Reward: -5.7800904466959775 Epsilon: 15.620118712902219\n",
            "Total Timesteps: 3212 Episode Num: 292 Reward: -4.104630524239227 Epsilon: 15.566625155666252\n",
            "Total Timesteps: 3223 Episode Num: 293 Reward: -5.885015809378377 Epsilon: 15.513496742165684\n",
            "Total Timesteps: 3234 Episode Num: 294 Reward: -5.4048627652262375 Epsilon: 15.460729746444033\n",
            "Total Timesteps: 3245 Episode Num: 295 Reward: -5.028038228654431 Epsilon: 15.408320493066256\n",
            "Total Timesteps: 3256 Episode Num: 296 Reward: -6.305527181913639 Epsilon: 15.356265356265355\n",
            "Total Timesteps: 3267 Episode Num: 297 Reward: -5.72776550589422 Epsilon: 15.304560759106213\n",
            "Total Timesteps: 3278 Episode Num: 298 Reward: -3.7503076384229193 Epsilon: 15.25320317266626\n",
            "Total Timesteps: 3289 Episode Num: 299 Reward: -5.631437663122254 Epsilon: 15.202189115232594\n",
            "Total Timesteps: 3300 Episode Num: 300 Reward: -4.940444069866139 Epsilon: 15.151515151515152\n",
            "Total Timesteps: 3311 Episode Num: 301 Reward: -5.101827457746218 Epsilon: 15.101177891875567\n",
            "Total Timesteps: 3322 Episode Num: 302 Reward: -5.137966413120005 Epsilon: 15.051173991571343\n",
            "Total Timesteps: 3333 Episode Num: 303 Reward: -4.769461081801861 Epsilon: 15.001500150015001\n",
            "Total Timesteps: 3344 Episode Num: 304 Reward: -6.314931461656346 Epsilon: 14.952153110047847\n",
            "Total Timesteps: 3355 Episode Num: 305 Reward: -5.073906751185669 Epsilon: 14.903129657228018\n",
            "Total Timesteps: 3366 Episode Num: 306 Reward: -6.324951306299841 Epsilon: 14.854426619132502\n",
            "Total Timesteps: 3377 Episode Num: 307 Reward: -6.088473226437407 Epsilon: 14.806040864672786\n",
            "Total Timesteps: 3388 Episode Num: 308 Reward: -7.266399978146779 Epsilon: 14.757969303423849\n",
            "Total Timesteps: 3399 Episode Num: 309 Reward: -5.057600912725919 Epsilon: 14.710208884966166\n",
            "Total Timesteps: 3410 Episode Num: 310 Reward: -5.546043874236014 Epsilon: 14.662756598240469\n",
            "Total Timesteps: 3421 Episode Num: 311 Reward: -6.081328090785078 Epsilon: 14.615609470914936\n",
            "Total Timesteps: 3432 Episode Num: 312 Reward: -6.161884371217436 Epsilon: 14.56876456876457\n",
            "Total Timesteps: 3443 Episode Num: 313 Reward: -5.141188292898122 Epsilon: 14.522218995062445\n",
            "Total Timesteps: 3454 Episode Num: 314 Reward: -5.932846822533012 Epsilon: 14.475969889982629\n",
            "Total Timesteps: 3465 Episode Num: 315 Reward: -7.750505425333059 Epsilon: 14.43001443001443\n",
            "Total Timesteps: 3476 Episode Num: 316 Reward: -5.416900426822374 Epsilon: 14.384349827387801\n",
            "Total Timesteps: 3487 Episode Num: 317 Reward: -5.360509124107336 Epsilon: 14.338973329509606\n",
            "Total Timesteps: 3498 Episode Num: 318 Reward: -6.6632653536481286 Epsilon: 14.29388221841052\n",
            "Total Timesteps: 3509 Episode Num: 319 Reward: -5.805591824279904 Epsilon: 14.249073810202336\n",
            "Total Timesteps: 3520 Episode Num: 320 Reward: -4.682554103667028 Epsilon: 14.204545454545455\n",
            "Total Timesteps: 3531 Episode Num: 321 Reward: -4.962181274281176 Epsilon: 14.16029453412631\n",
            "Total Timesteps: 3542 Episode Num: 322 Reward: -6.192685750572082 Epsilon: 14.116318464144552\n",
            "Total Timesteps: 3553 Episode Num: 323 Reward: -5.3943682698974005 Epsilon: 14.072614691809738\n",
            "Total Timesteps: 3564 Episode Num: 324 Reward: -6.137004026563481 Epsilon: 14.029180695847362\n",
            "Total Timesteps: 3575 Episode Num: 325 Reward: -4.064272798965627 Epsilon: 13.986013986013987\n",
            "Total Timesteps: 3586 Episode Num: 326 Reward: -6.0242013085648995 Epsilon: 13.943112102621305\n",
            "Total Timesteps: 3597 Episode Num: 327 Reward: -4.9952583489527065 Epsilon: 13.900472616068946\n",
            "Total Timesteps: 3608 Episode Num: 328 Reward: -6.269634644368558 Epsilon: 13.858093126385809\n",
            "Total Timesteps: 3619 Episode Num: 329 Reward: -6.063764307438936 Epsilon: 13.815971262779774\n",
            "Total Timesteps: 3630 Episode Num: 330 Reward: -3.8702411829255503 Epsilon: 13.774104683195592\n",
            "Total Timesteps: 3641 Episode Num: 331 Reward: -5.180420903883902 Epsilon: 13.732491073880801\n",
            "Total Timesteps: 3652 Episode Num: 332 Reward: -6.158331015010411 Epsilon: 13.691128148959475\n",
            "Total Timesteps: 3663 Episode Num: 333 Reward: -4.7137766019227065 Epsilon: 13.65001365001365\n",
            "Total Timesteps: 3674 Episode Num: 334 Reward: -5.388345702646033 Epsilon: 13.609145345672292\n",
            "Total Timesteps: 3685 Episode Num: 335 Reward: -5.03622825823805 Epsilon: 13.568521031207599\n",
            "Total Timesteps: 3696 Episode Num: 336 Reward: -6.242755238359075 Epsilon: 13.528138528138529\n",
            "Total Timesteps: 3707 Episode Num: 337 Reward: -5.888135147406658 Epsilon: 13.487995683841381\n",
            "Total Timesteps: 3718 Episode Num: 338 Reward: -6.279904868299332 Epsilon: 13.448090371167295\n",
            "Total Timesteps: 3729 Episode Num: 339 Reward: -4.865332628114624 Epsilon: 13.408420488066506\n",
            "Total Timesteps: 3740 Episode Num: 340 Reward: -7.690733013696981 Epsilon: 13.368983957219251\n",
            "Total Timesteps: 3751 Episode Num: 341 Reward: -6.350938426738995 Epsilon: 13.329778725673155\n",
            "Total Timesteps: 3762 Episode Num: 342 Reward: -5.990632380173136 Epsilon: 13.290802764486974\n",
            "Total Timesteps: 3773 Episode Num: 343 Reward: -5.596033132306887 Epsilon: 13.252054068380598\n",
            "Total Timesteps: 3784 Episode Num: 344 Reward: -4.247024458786016 Epsilon: 13.213530655391121\n",
            "Total Timesteps: 3795 Episode Num: 345 Reward: -4.545368586957929 Epsilon: 13.175230566534914\n",
            "Total Timesteps: 3806 Episode Num: 346 Reward: -6.130870118911828 Epsilon: 13.137151865475564\n",
            "Total Timesteps: 3817 Episode Num: 347 Reward: -5.223706961581852 Epsilon: 13.099292638197538\n",
            "Total Timesteps: 3828 Episode Num: 348 Reward: -5.498457047406014 Epsilon: 13.061650992685475\n",
            "Total Timesteps: 3839 Episode Num: 349 Reward: -6.838152269850591 Epsilon: 13.024225058609012\n",
            "Total Timesteps: 3850 Episode Num: 350 Reward: -6.987991334748725 Epsilon: 12.987012987012987\n",
            "Total Timesteps: 3861 Episode Num: 351 Reward: -6.1471864476747085 Epsilon: 12.95001295001295\n",
            "Total Timesteps: 3872 Episode Num: 352 Reward: -6.351631596802038 Epsilon: 12.913223140495868\n",
            "Total Timesteps: 3883 Episode Num: 353 Reward: -6.444762453259346 Epsilon: 12.876641771825907\n",
            "Total Timesteps: 3894 Episode Num: 354 Reward: -7.534970437555339 Epsilon: 12.840267077555213\n",
            "Total Timesteps: 3905 Episode Num: 355 Reward: -5.516415628224754 Epsilon: 12.804097311139564\n",
            "Total Timesteps: 3916 Episode Num: 356 Reward: -4.929966541460779 Epsilon: 12.768130745658835\n",
            "Total Timesteps: 3927 Episode Num: 357 Reward: -7.663474867772741 Epsilon: 12.732365673542144\n",
            "Total Timesteps: 3938 Episode Num: 358 Reward: -6.830208894140295 Epsilon: 12.696800406297614\n",
            "Total Timesteps: 3949 Episode Num: 359 Reward: -5.428543070613898 Epsilon: 12.661433274246646\n",
            "Total Timesteps: 3960 Episode Num: 360 Reward: -4.519248545318609 Epsilon: 12.626262626262626\n",
            "Total Timesteps: 3971 Episode Num: 361 Reward: -6.473870934482501 Epsilon: 12.591286829513976\n",
            "Total Timesteps: 3982 Episode Num: 362 Reward: -6.522785134926514 Epsilon: 12.556504269211452\n",
            "Total Timesteps: 3993 Episode Num: 363 Reward: -7.136338015491917 Epsilon: 12.521913348359629\n",
            "Total Timesteps: 4004 Episode Num: 364 Reward: -8.274639030694686 Epsilon: 12.487512487512488\n",
            "Total Timesteps: 4015 Episode Num: 365 Reward: -6.18474948701253 Epsilon: 12.453300124533001\n",
            "Total Timesteps: 4026 Episode Num: 366 Reward: -6.153913541913213 Epsilon: 12.419274714356682\n",
            "Total Timesteps: 4037 Episode Num: 367 Reward: -5.067059835681965 Epsilon: 12.38543472875898\n",
            "Total Timesteps: 4048 Episode Num: 368 Reward: -4.182898037439424 Epsilon: 12.351778656126482\n",
            "Total Timesteps: 4059 Episode Num: 369 Reward: -4.649481488782365 Epsilon: 12.318305001231831\n",
            "Total Timesteps: 4070 Episode Num: 370 Reward: -6.273353716867511 Epsilon: 12.285012285012286\n",
            "Total Timesteps: 4081 Episode Num: 371 Reward: -4.79326920381763 Epsilon: 12.251899044351875\n",
            "Total Timesteps: 4092 Episode Num: 372 Reward: -4.952752898401488 Epsilon: 12.218963831867057\n",
            "Total Timesteps: 4103 Episode Num: 373 Reward: -6.021841291310484 Epsilon: 12.186205215695832\n",
            "Total Timesteps: 4114 Episode Num: 374 Reward: -5.66443057535217 Epsilon: 12.153621779290228\n",
            "Total Timesteps: 4125 Episode Num: 375 Reward: -4.510418791235319 Epsilon: 12.121212121212121\n",
            "Total Timesteps: 4136 Episode Num: 376 Reward: -5.9999028025438506 Epsilon: 12.088974854932301\n",
            "Total Timesteps: 4147 Episode Num: 377 Reward: -5.110578794789267 Epsilon: 12.056908608632746\n",
            "Total Timesteps: 4158 Episode Num: 378 Reward: -5.972715895298042 Epsilon: 12.025012025012025\n",
            "Total Timesteps: 4169 Episode Num: 379 Reward: -5.8369182808456275 Epsilon: 11.993283761093787\n",
            "Total Timesteps: 4180 Episode Num: 380 Reward: -5.75444251800547 Epsilon: 11.961722488038278\n",
            "Total Timesteps: 4191 Episode Num: 381 Reward: -5.244244121217164 Epsilon: 11.930326890956811\n",
            "Total Timesteps: 4202 Episode Num: 382 Reward: -6.587369525699249 Epsilon: 11.899095668729176\n",
            "Total Timesteps: 4213 Episode Num: 383 Reward: -5.780217372827476 Epsilon: 11.868027533823879\n",
            "Total Timesteps: 4224 Episode Num: 384 Reward: -4.975862113291868 Epsilon: 11.837121212121213\n",
            "Total Timesteps: 4235 Episode Num: 385 Reward: -5.345334563420783 Epsilon: 11.806375442739078\n",
            "Total Timesteps: 4246 Episode Num: 386 Reward: -4.1081714862882235 Epsilon: 11.775788977861517\n",
            "Total Timesteps: 4257 Episode Num: 387 Reward: -6.12779980592124 Epsilon: 11.745360582569885\n",
            "Total Timesteps: 4268 Episode Num: 388 Reward: -7.094906973151503 Epsilon: 11.715089034676664\n",
            "Total Timesteps: 4279 Episode Num: 389 Reward: -5.221339951720817 Epsilon: 11.684973124561813\n",
            "Total Timesteps: 4290 Episode Num: 390 Reward: -6.208631600410622 Epsilon: 11.655011655011656\n",
            "Total Timesteps: 4301 Episode Num: 391 Reward: -6.768007120264383 Epsilon: 11.625203441060219\n",
            "Total Timesteps: 4312 Episode Num: 392 Reward: -4.785153145569002 Epsilon: 11.595547309833025\n",
            "Total Timesteps: 4323 Episode Num: 393 Reward: -5.473154218043834 Epsilon: 11.566042100393245\n",
            "Total Timesteps: 4334 Episode Num: 394 Reward: -6.316547334916572 Epsilon: 11.536686663590217\n",
            "Total Timesteps: 4345 Episode Num: 395 Reward: -5.013270161327044 Epsilon: 11.507479861910241\n",
            "Total Timesteps: 4356 Episode Num: 396 Reward: -4.044585312501413 Epsilon: 11.47842056932966\n",
            "Total Timesteps: 4367 Episode Num: 397 Reward: -5.170026131753989 Epsilon: 11.44950767117014\n",
            "Total Timesteps: 4378 Episode Num: 398 Reward: -4.9947720733131815 Epsilon: 11.420740063956144\n",
            "Total Timesteps: 4389 Episode Num: 399 Reward: -6.805960513401075 Epsilon: 11.39211665527455\n",
            "Total Timesteps: 4400 Episode Num: 400 Reward: -5.25568023303171 Epsilon: 11.363636363636363\n",
            "Total Timesteps: 4411 Episode Num: 401 Reward: -5.561640985225629 Epsilon: 11.335298118340512\n",
            "Total Timesteps: 4422 Episode Num: 402 Reward: -6.869975601319001 Epsilon: 11.307100859339664\n",
            "Total Timesteps: 4433 Episode Num: 403 Reward: -6.318779310241653 Epsilon: 11.279043537108054\n",
            "Total Timesteps: 4444 Episode Num: 404 Reward: -4.878490107971854 Epsilon: 11.251125112511252\n",
            "Total Timesteps: 4455 Episode Num: 405 Reward: -6.350418452010753 Epsilon: 11.22334455667789\n",
            "Total Timesteps: 4466 Episode Num: 406 Reward: -6.45173632916438 Epsilon: 11.195700850873264\n",
            "Total Timesteps: 4477 Episode Num: 407 Reward: -6.401336673281701 Epsilon: 11.168192986374805\n",
            "Total Timesteps: 4488 Episode Num: 408 Reward: -5.215750090814895 Epsilon: 11.140819964349376\n",
            "Total Timesteps: 4499 Episode Num: 409 Reward: -4.34453792384168 Epsilon: 11.113580795732386\n",
            "Total Timesteps: 4510 Episode Num: 410 Reward: -4.779410924974708 Epsilon: 11.086474501108647\n",
            "Total Timesteps: 4521 Episode Num: 411 Reward: -5.9570349473239075 Epsilon: 11.059500110595001\n",
            "Total Timesteps: 4532 Episode Num: 412 Reward: -5.216107068179731 Epsilon: 11.032656663724625\n",
            "Total Timesteps: 4543 Episode Num: 413 Reward: -4.397631581721549 Epsilon: 11.00594320933304\n",
            "Total Timesteps: 4554 Episode Num: 414 Reward: -6.996370359524677 Epsilon: 10.979358805445761\n",
            "Total Timesteps: 4565 Episode Num: 415 Reward: -4.173978966673306 Epsilon: 10.95290251916758\n",
            "Total Timesteps: 4576 Episode Num: 416 Reward: -5.455833265492806 Epsilon: 10.926573426573427\n",
            "Total Timesteps: 4587 Episode Num: 417 Reward: -4.303063527763693 Epsilon: 10.90037061260083\n",
            "Total Timesteps: 4598 Episode Num: 418 Reward: -5.473239828447839 Epsilon: 10.874293170943888\n",
            "Total Timesteps: 4609 Episode Num: 419 Reward: -4.901682060421107 Epsilon: 10.848340203948796\n",
            "Total Timesteps: 4620 Episode Num: 420 Reward: -4.848719609966132 Epsilon: 10.822510822510823\n",
            "Total Timesteps: 4631 Episode Num: 421 Reward: -7.687067382442466 Epsilon: 10.796804145972793\n",
            "Total Timesteps: 4642 Episode Num: 422 Reward: -4.044377836097062 Epsilon: 10.771219302024988\n",
            "Total Timesteps: 4653 Episode Num: 423 Reward: -5.99505301477348 Epsilon: 10.74575542660649\n",
            "Total Timesteps: 4664 Episode Num: 424 Reward: -6.514129402330319 Epsilon: 10.72041166380789\n",
            "Total Timesteps: 4675 Episode Num: 425 Reward: -4.664912825996678 Epsilon: 10.695187165775401\n",
            "Total Timesteps: 4686 Episode Num: 426 Reward: -5.421097685022101 Epsilon: 10.670081092616304\n",
            "Total Timesteps: 4697 Episode Num: 427 Reward: -5.089495002310342 Epsilon: 10.645092612305728\n",
            "Total Timesteps: 4708 Episode Num: 428 Reward: -7.361953088707077 Epsilon: 10.620220900594733\n",
            "Total Timesteps: 4719 Episode Num: 429 Reward: -6.430477171808529 Epsilon: 10.595465140919686\n",
            "Total Timesteps: 4730 Episode Num: 430 Reward: -6.87804446666106 Epsilon: 10.570824524312897\n",
            "Total Timesteps: 4741 Episode Num: 431 Reward: -5.541071700713133 Epsilon: 10.546298249314491\n",
            "Total Timesteps: 4752 Episode Num: 432 Reward: -5.65769613633696 Epsilon: 10.521885521885523\n",
            "Total Timesteps: 4763 Episode Num: 433 Reward: -4.00130067665595 Epsilon: 10.497585555322276\n",
            "Total Timesteps: 4774 Episode Num: 434 Reward: -6.415602630723376 Epsilon: 10.473397570171764\n",
            "Total Timesteps: 4785 Episode Num: 435 Reward: -6.475067200518 Epsilon: 10.449320794148381\n",
            "Total Timesteps: 4796 Episode Num: 436 Reward: -4.519676007425159 Epsilon: 10.42535446205171\n",
            "Total Timesteps: 4807 Episode Num: 437 Reward: -6.8312284438019315 Epsilon: 10.401497815685458\n",
            "Total Timesteps: 4818 Episode Num: 438 Reward: -5.657749749414312 Epsilon: 10.377750103777501\n",
            "Total Timesteps: 4829 Episode Num: 439 Reward: -6.39939181074351 Epsilon: 10.354110581901015\n",
            "Total Timesteps: 4840 Episode Num: 440 Reward: -5.17475424628857 Epsilon: 10.330578512396695\n",
            "Total Timesteps: 4851 Episode Num: 441 Reward: -4.906600725405161 Epsilon: 10.307153164296022\n",
            "Total Timesteps: 4862 Episode Num: 442 Reward: -7.0097940291832295 Epsilon: 10.283833813245577\n",
            "Total Timesteps: 4873 Episode Num: 443 Reward: -5.74561119050014 Epsilon: 10.260619741432382\n",
            "Total Timesteps: 4884 Episode Num: 444 Reward: -4.63022493779731 Epsilon: 10.237510237510238\n",
            "Total Timesteps: 4895 Episode Num: 445 Reward: -6.068995818163525 Epsilon: 10.21450459652707\n",
            "Total Timesteps: 4906 Episode Num: 446 Reward: -5.726817583453522 Epsilon: 10.191602119853242\n",
            "Total Timesteps: 4917 Episode Num: 447 Reward: -5.766164873003892 Epsilon: 10.16880211511084\n",
            "Total Timesteps: 4928 Episode Num: 448 Reward: -4.070263735387266 Epsilon: 10.146103896103897\n",
            "Total Timesteps: 4939 Episode Num: 449 Reward: -5.922259719930847 Epsilon: 10.123506782749544\n",
            "Total Timesteps: 4950 Episode Num: 450 Reward: -5.887754537435146 Epsilon: 10.1010101010101\n",
            "Total Timesteps: 4961 Episode Num: 451 Reward: -7.174996297052905 Epsilon: 10.078613182826043\n",
            "Total Timesteps: 4972 Episode Num: 452 Reward: -5.098262481071773 Epsilon: 10.05631536604988\n",
            "Total Timesteps: 4983 Episode Num: 453 Reward: -5.290901712097332 Epsilon: 10.034115994380896\n",
            "Total Timesteps: 4994 Episode Num: 454 Reward: -6.103845209464748 Epsilon: 10.012014417300762\n",
            "Total Timesteps: 5005 Episode Num: 455 Reward: -4.95335851389373 Epsilon: 9.99000999000999\n",
            "Total timesteps: 5005 Epsilon: 1\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -5.529451\n",
            "---------------------------------------\n",
            "Total Timesteps: 5016 Episode Num: 456 Reward: -5.604932615127944 Epsilon: 9.968102073365232\n",
            "Total Timesteps: 5027 Episode Num: 457 Reward: -5.399627375734869 Epsilon: 9.946290033817386\n",
            "Total Timesteps: 5038 Episode Num: 458 Reward: -5.56370370468388 Epsilon: 9.924573243350537\n",
            "Total Timesteps: 5049 Episode Num: 459 Reward: -6.474528872285965 Epsilon: 9.902951079421667\n",
            "Total Timesteps: 5060 Episode Num: 460 Reward: -4.3386183840073755 Epsilon: 9.881422924901186\n",
            "Total Timesteps: 5071 Episode Num: 461 Reward: -5.7980357342998134 Epsilon: 9.859988168014198\n",
            "Total Timesteps: 5082 Episode Num: 462 Reward: -6.368847317085753 Epsilon: 9.838646202282566\n",
            "Total Timesteps: 5093 Episode Num: 463 Reward: -6.584725472973061 Epsilon: 9.8173964264677\n",
            "Total Timesteps: 5104 Episode Num: 464 Reward: -6.273912932198597 Epsilon: 9.796238244514107\n",
            "Total Timesteps: 5115 Episode Num: 465 Reward: -5.347263522937164 Epsilon: 9.775171065493646\n",
            "Total Timesteps: 5126 Episode Num: 466 Reward: -5.659467976885903 Epsilon: 9.754194303550527\n",
            "Total Timesteps: 5137 Episode Num: 467 Reward: -4.737364223986002 Epsilon: 9.733307377846993\n",
            "Total Timesteps: 5148 Episode Num: 468 Reward: -6.923215812448533 Epsilon: 9.712509712509712\n",
            "Total Timesteps: 5159 Episode Num: 469 Reward: -6.259564723712677 Epsilon: 9.691800736576855\n",
            "Total Timesteps: 5170 Episode Num: 470 Reward: -6.087653437389689 Epsilon: 9.671179883945841\n",
            "Total Timesteps: 5181 Episode Num: 471 Reward: -5.722362370913489 Epsilon: 9.650646593321753\n",
            "Total Timesteps: 5192 Episode Num: 472 Reward: -5.425575602914844 Epsilon: 9.63020030816641\n",
            "Total Timesteps: 5203 Episode Num: 473 Reward: -6.934767102848448 Epsilon: 9.609840476648088\n",
            "Total Timesteps: 5214 Episode Num: 474 Reward: -5.655488073154136 Epsilon: 9.589566551591869\n",
            "Total Timesteps: 5225 Episode Num: 475 Reward: -5.633288657781851 Epsilon: 9.569377990430622\n",
            "Total Timesteps: 5236 Episode Num: 476 Reward: -5.469745483281057 Epsilon: 9.549274255156607\n",
            "Total Timesteps: 5247 Episode Num: 477 Reward: -5.801241624335561 Epsilon: 9.52925481227368\n",
            "Total Timesteps: 5258 Episode Num: 478 Reward: -5.331637645932391 Epsilon: 9.509319132750095\n",
            "Total Timesteps: 5269 Episode Num: 479 Reward: -6.403542697635572 Epsilon: 9.489466691971911\n",
            "Total Timesteps: 5280 Episode Num: 480 Reward: -4.6046752467865435 Epsilon: 9.469696969696969\n",
            "Total Timesteps: 5291 Episode Num: 481 Reward: -5.417823644925997 Epsilon: 9.45000945000945\n",
            "Total Timesteps: 5302 Episode Num: 482 Reward: -4.01334534301507 Epsilon: 9.43040362127499\n",
            "Total Timesteps: 5313 Episode Num: 483 Reward: -5.788150490521135 Epsilon: 9.410878976096367\n",
            "Total Timesteps: 5324 Episode Num: 484 Reward: -6.133980124472194 Epsilon: 9.391435011269722\n",
            "Total Timesteps: 5335 Episode Num: 485 Reward: -6.654658295262472 Epsilon: 9.372071227741332\n",
            "Total Timesteps: 5346 Episode Num: 486 Reward: -4.141089828163798 Epsilon: 9.352787130564908\n",
            "Total Timesteps: 5357 Episode Num: 487 Reward: -4.218352458141451 Epsilon: 9.333582228859436\n",
            "Total Timesteps: 5368 Episode Num: 488 Reward: -7.603732952663945 Epsilon: 9.314456035767511\n",
            "Total Timesteps: 5379 Episode Num: 489 Reward: -5.602810007779527 Epsilon: 9.295408068414204\n",
            "Total Timesteps: 5390 Episode Num: 490 Reward: -5.568487040139908 Epsilon: 9.276437847866418\n",
            "Total Timesteps: 5401 Episode Num: 491 Reward: -5.515529358411698 Epsilon: 9.25754489909276\n",
            "Total Timesteps: 5412 Episode Num: 492 Reward: -5.836888481058615 Epsilon: 9.238728750923872\n",
            "Total Timesteps: 5423 Episode Num: 493 Reward: -6.298141753003594 Epsilon: 9.219988936013277\n",
            "Total Timesteps: 5434 Episode Num: 494 Reward: -5.852331189973276 Epsilon: 9.201324990798675\n",
            "Total Timesteps: 5445 Episode Num: 495 Reward: -6.289593316994652 Epsilon: 9.182736455463727\n",
            "Total Timesteps: 5456 Episode Num: 496 Reward: -5.621707748110434 Epsilon: 9.164222873900293\n",
            "Total Timesteps: 5467 Episode Num: 497 Reward: -5.348793815617519 Epsilon: 9.145783793671118\n",
            "Total Timesteps: 5478 Episode Num: 498 Reward: -5.686120521294383 Epsilon: 9.127418765972983\n",
            "Total Timesteps: 5489 Episode Num: 499 Reward: -6.2972126252670035 Epsilon: 9.109127345600292\n",
            "Total Timesteps: 5500 Episode Num: 500 Reward: -6.0429398163261325 Epsilon: 9.090909090909092\n",
            "Total Timesteps: 5511 Episode Num: 501 Reward: -6.321493737212502 Epsilon: 9.072763563781528\n",
            "Total Timesteps: 5522 Episode Num: 502 Reward: -5.680266561450186 Epsilon: 9.054690329590729\n",
            "Total Timesteps: 5533 Episode Num: 503 Reward: -4.002940140363749 Epsilon: 9.036688957166094\n",
            "Total Timesteps: 5544 Episode Num: 504 Reward: -5.85405081222836 Epsilon: 9.018759018759019\n",
            "Total Timesteps: 5555 Episode Num: 505 Reward: -6.909130013487323 Epsilon: 9.000900090009\n",
            "Total Timesteps: 5566 Episode Num: 506 Reward: -3.3308810906114723 Epsilon: 8.983111749910169\n",
            "Total Timesteps: 5577 Episode Num: 507 Reward: -8.574214581681536 Epsilon: 8.965393580778196\n",
            "Total Timesteps: 5588 Episode Num: 508 Reward: -5.850988980840135 Epsilon: 8.94774516821761\n",
            "Total Timesteps: 5599 Episode Num: 509 Reward: -4.438118701139686 Epsilon: 8.93016610108948\n",
            "Total Timesteps: 5610 Episode Num: 510 Reward: -6.52138907963864 Epsilon: 8.9126559714795\n",
            "Total Timesteps: 5621 Episode Num: 511 Reward: -5.421101311179873 Epsilon: 8.895214374666429\n",
            "Total Timesteps: 5632 Episode Num: 512 Reward: -5.661043674112778 Epsilon: 8.877840909090908\n",
            "Total Timesteps: 5643 Episode Num: 513 Reward: -7.329857538326025 Epsilon: 8.86053517632465\n",
            "Total Timesteps: 5654 Episode Num: 514 Reward: -5.695371796342031 Epsilon: 8.843296781039971\n",
            "Total Timesteps: 5665 Episode Num: 515 Reward: -5.201254019982522 Epsilon: 8.8261253309797\n",
            "Total Timesteps: 5676 Episode Num: 516 Reward: -4.456344740532482 Epsilon: 8.809020436927414\n",
            "Total Timesteps: 5687 Episode Num: 517 Reward: -4.364759447553526 Epsilon: 8.791981712678037\n",
            "Total Timesteps: 5698 Episode Num: 518 Reward: -5.833354669073995 Epsilon: 8.775008775008775\n",
            "Total Timesteps: 5709 Episode Num: 519 Reward: -5.240010865776288 Epsilon: 8.758101243650376\n",
            "Total Timesteps: 5720 Episode Num: 520 Reward: -4.60275728539787 Epsilon: 8.741258741258742\n",
            "Total Timesteps: 5731 Episode Num: 521 Reward: -7.318327311171344 Epsilon: 8.724480893386843\n",
            "Total Timesteps: 5742 Episode Num: 522 Reward: -5.0261220621147675 Epsilon: 8.707767328456983\n",
            "Total Timesteps: 5753 Episode Num: 523 Reward: -6.266556374963499 Epsilon: 8.691117677733356\n",
            "Total Timesteps: 5764 Episode Num: 524 Reward: -5.72905949859998 Epsilon: 8.674531575294933\n",
            "Total Timesteps: 5775 Episode Num: 525 Reward: -5.43671112254603 Epsilon: 8.658008658008658\n",
            "Total Timesteps: 5786 Episode Num: 526 Reward: -7.5149434692588155 Epsilon: 8.641548565502939\n",
            "Total Timesteps: 5797 Episode Num: 527 Reward: -6.094887979156676 Epsilon: 8.625150940141452\n",
            "Total Timesteps: 5808 Episode Num: 528 Reward: -5.850333109600024 Epsilon: 8.608815426997245\n",
            "Total Timesteps: 5819 Episode Num: 529 Reward: -6.407144636800001 Epsilon: 8.592541673827117\n",
            "Total Timesteps: 5830 Episode Num: 530 Reward: -6.885327419119607 Epsilon: 8.576329331046312\n",
            "Total Timesteps: 5841 Episode Num: 531 Reward: -6.078621018307621 Epsilon: 8.560178051703476\n",
            "Total Timesteps: 5852 Episode Num: 532 Reward: -5.651803612669723 Epsilon: 8.544087491455912\n",
            "Total Timesteps: 5863 Episode Num: 533 Reward: -5.963629915392798 Epsilon: 8.528057308545113\n",
            "Total Timesteps: 5874 Episode Num: 534 Reward: -5.882560996114664 Epsilon: 8.512087163772557\n",
            "Total Timesteps: 5885 Episode Num: 535 Reward: -4.817761346330887 Epsilon: 8.496176720475786\n",
            "Total Timesteps: 5896 Episode Num: 536 Reward: -3.940645551724495 Epsilon: 8.480325644504749\n",
            "Total Timesteps: 5907 Episode Num: 537 Reward: -6.940224797964241 Epsilon: 8.464533604198408\n",
            "Total Timesteps: 5918 Episode Num: 538 Reward: -4.8886961781120375 Epsilon: 8.448800270361609\n",
            "Total Timesteps: 5929 Episode Num: 539 Reward: -5.474983769499078 Epsilon: 8.433125316242199\n",
            "Total Timesteps: 5940 Episode Num: 540 Reward: -6.138880911244413 Epsilon: 8.417508417508417\n",
            "Total Timesteps: 5951 Episode Num: 541 Reward: -5.964824294862834 Epsilon: 8.401949252226517\n",
            "Total Timesteps: 5962 Episode Num: 542 Reward: -5.7737766772632195 Epsilon: 8.386447500838644\n",
            "Total Timesteps: 5973 Episode Num: 543 Reward: -6.731294519422982 Epsilon: 8.371002846140968\n",
            "Total Timesteps: 5984 Episode Num: 544 Reward: -5.626977102032707 Epsilon: 8.355614973262032\n",
            "Total Timesteps: 5995 Episode Num: 545 Reward: -4.569711025679732 Epsilon: 8.340283569641368\n",
            "Total Timesteps: 6006 Episode Num: 546 Reward: -4.503420799590323 Epsilon: 8.325008325008325\n",
            "Total Timesteps: 6017 Episode Num: 547 Reward: -6.052235216359338 Epsilon: 8.309788931361144\n",
            "Total Timesteps: 6028 Episode Num: 548 Reward: -5.718525374696128 Epsilon: 8.29462508294625\n",
            "Total Timesteps: 6039 Episode Num: 549 Reward: -5.925971824081106 Epsilon: 8.279516476237788\n",
            "Total Timesteps: 6050 Episode Num: 550 Reward: -6.3350099561327 Epsilon: 8.264462809917354\n",
            "Total Timesteps: 6061 Episode Num: 551 Reward: -5.713015508725009 Epsilon: 8.249463784853985\n",
            "Total Timesteps: 6072 Episode Num: 552 Reward: -3.998710769126079 Epsilon: 8.234519104084322\n",
            "Total Timesteps: 6083 Episode Num: 553 Reward: -6.590156245061506 Epsilon: 8.21962847279303\n",
            "Total Timesteps: 6094 Episode Num: 554 Reward: -5.294856359918812 Epsilon: 8.204791598293403\n",
            "Total Timesteps: 6105 Episode Num: 555 Reward: -5.899916920153553 Epsilon: 8.19000819000819\n",
            "Total Timesteps: 6116 Episode Num: 556 Reward: -7.093492774497877 Epsilon: 8.175277959450622\n",
            "Total Timesteps: 6127 Episode Num: 557 Reward: -7.115464061169606 Epsilon: 8.160600620205647\n",
            "Total Timesteps: 6138 Episode Num: 558 Reward: -6.513084137147957 Epsilon: 8.145975887911371\n",
            "Total Timesteps: 6149 Episode Num: 559 Reward: -4.772466562613317 Epsilon: 8.131403480240689\n",
            "Total Timesteps: 6160 Episode Num: 560 Reward: -7.31399176467735 Epsilon: 8.116883116883116\n",
            "Total Timesteps: 6171 Episode Num: 561 Reward: -4.360415835853341 Epsilon: 8.10241451952682\n",
            "Total Timesteps: 6182 Episode Num: 562 Reward: -4.398677843722256 Epsilon: 8.087997411840828\n",
            "Total Timesteps: 6193 Episode Num: 563 Reward: -4.212387923651321 Epsilon: 8.073631519457452\n",
            "Total Timesteps: 6204 Episode Num: 564 Reward: -5.995705102360684 Epsilon: 8.059316569954868\n",
            "Total Timesteps: 6215 Episode Num: 565 Reward: -5.32520821902612 Epsilon: 8.045052292839904\n",
            "Total Timesteps: 6226 Episode Num: 566 Reward: -5.955792414613176 Epsilon: 8.030838419531\n",
            "Total Timesteps: 6237 Episode Num: 567 Reward: -5.478946822683058 Epsilon: 8.01667468334135\n",
            "Total Timesteps: 6248 Episode Num: 568 Reward: -7.412256335526215 Epsilon: 8.002560819462229\n",
            "Total Timesteps: 6259 Episode Num: 569 Reward: -5.145866920021489 Epsilon: 7.988496564946477\n",
            "Total Timesteps: 6270 Episode Num: 570 Reward: -5.353294173298194 Epsilon: 7.974481658692185\n",
            "Total Timesteps: 6281 Episode Num: 571 Reward: -3.9124119177019128 Epsilon: 7.960515841426524\n",
            "Total Timesteps: 6292 Episode Num: 572 Reward: -4.945039321720559 Epsilon: 7.946598855689765\n",
            "Total Timesteps: 6303 Episode Num: 573 Reward: -6.009659583882655 Epsilon: 7.932730445819451\n",
            "Total Timesteps: 6314 Episode Num: 574 Reward: -5.599913182518161 Epsilon: 7.918910357934748\n",
            "Total Timesteps: 6325 Episode Num: 575 Reward: -5.968590529495865 Epsilon: 7.905138339920948\n",
            "Total Timesteps: 6336 Episode Num: 576 Reward: -5.834172404125919 Epsilon: 7.891414141414141\n",
            "Total Timesteps: 6347 Episode Num: 577 Reward: -6.347277960443037 Epsilon: 7.877737513786041\n",
            "Total Timesteps: 6358 Episode Num: 578 Reward: -6.462760169096809 Epsilon: 7.864108210128971\n",
            "Total Timesteps: 6369 Episode Num: 579 Reward: -5.166218870042931 Epsilon: 7.850525985241011\n",
            "Total Timesteps: 6380 Episode Num: 580 Reward: -7.972164379415821 Epsilon: 7.836990595611285\n",
            "Total Timesteps: 6391 Episode Num: 581 Reward: -3.433993530962325 Epsilon: 7.823501799405414\n",
            "Total Timesteps: 6402 Episode Num: 582 Reward: -6.362513174252193 Epsilon: 7.810059356451109\n",
            "Total Timesteps: 6413 Episode Num: 583 Reward: -4.334176629485198 Epsilon: 7.79666302822392\n",
            "Total Timesteps: 6424 Episode Num: 584 Reward: -4.617316756793511 Epsilon: 7.783312577833126\n",
            "Total Timesteps: 6435 Episode Num: 585 Reward: -5.2737525495367805 Epsilon: 7.77000777000777\n",
            "Total Timesteps: 6446 Episode Num: 586 Reward: -5.591492519576603 Epsilon: 7.756748371082842\n",
            "Total Timesteps: 6457 Episode Num: 587 Reward: -5.213873990624916 Epsilon: 7.743534148985597\n",
            "Total Timesteps: 6468 Episode Num: 588 Reward: -4.413621431701781 Epsilon: 7.730364873222016\n",
            "Total Timesteps: 6479 Episode Num: 589 Reward: -5.535303239420202 Epsilon: 7.717240314863405\n",
            "Total Timesteps: 6490 Episode Num: 590 Reward: -7.014053230158649 Epsilon: 7.704160246533128\n",
            "Total Timesteps: 6501 Episode Num: 591 Reward: -5.932795762401907 Epsilon: 7.691124442393478\n",
            "Total Timesteps: 6512 Episode Num: 592 Reward: -5.062494499916567 Epsilon: 7.678132678132678\n",
            "Total Timesteps: 6523 Episode Num: 593 Reward: -5.028898221177617 Epsilon: 7.665184730952016\n",
            "Total Timesteps: 6534 Episode Num: 594 Reward: -5.5187737554560155 Epsilon: 7.6522803795531065\n",
            "Total Timesteps: 6545 Episode Num: 595 Reward: -6.512487302710779 Epsilon: 7.639419404125286\n",
            "Total Timesteps: 6556 Episode Num: 596 Reward: -6.320501590670718 Epsilon: 7.62660158633313\n",
            "Total Timesteps: 6567 Episode Num: 597 Reward: -6.9911903393814665 Epsilon: 7.613826709304096\n",
            "Total Timesteps: 6578 Episode Num: 598 Reward: -4.9216829694257145 Epsilon: 7.601094557616297\n",
            "Total Timesteps: 6589 Episode Num: 599 Reward: -4.429377282167336 Epsilon: 7.588404917286386\n",
            "Total Timesteps: 6600 Episode Num: 600 Reward: -5.787695809399058 Epsilon: 7.575757575757576\n",
            "Total Timesteps: 6611 Episode Num: 601 Reward: -6.61773581569885 Epsilon: 7.563152321887763\n",
            "Total Timesteps: 6622 Episode Num: 602 Reward: -4.254724117484916 Epsilon: 7.5505889459377835\n",
            "Total Timesteps: 6633 Episode Num: 603 Reward: -4.483543586039761 Epsilon: 7.538067239559777\n",
            "Total Timesteps: 6644 Episode Num: 604 Reward: -6.746617194921607 Epsilon: 7.525586995785671\n",
            "Total Timesteps: 6655 Episode Num: 605 Reward: -4.860998663533065 Epsilon: 7.513148009015778\n",
            "Total Timesteps: 6666 Episode Num: 606 Reward: -6.444116718867636 Epsilon: 7.500750075007501\n",
            "Total Timesteps: 6677 Episode Num: 607 Reward: -7.43880990306136 Epsilon: 7.488392990864161\n",
            "Total Timesteps: 6688 Episode Num: 608 Reward: -5.2352998442565895 Epsilon: 7.476076555023924\n",
            "Total Timesteps: 6699 Episode Num: 609 Reward: -5.006082058000005 Epsilon: 7.463800567248843\n",
            "Total Timesteps: 6710 Episode Num: 610 Reward: -5.6626878940378615 Epsilon: 7.451564828614009\n",
            "Total Timesteps: 6721 Episode Num: 611 Reward: -6.890788261324167 Epsilon: 7.439369141496801\n",
            "Total Timesteps: 6732 Episode Num: 612 Reward: -6.3455683632553 Epsilon: 7.427213309566251\n",
            "Total Timesteps: 6743 Episode Num: 613 Reward: -5.815950275286818 Epsilon: 7.415097137772505\n",
            "Total Timesteps: 6754 Episode Num: 614 Reward: -6.505579717188945 Epsilon: 7.403020432336393\n",
            "Total Timesteps: 6765 Episode Num: 615 Reward: -5.383455411527422 Epsilon: 7.390983000739098\n",
            "Total Timesteps: 6776 Episode Num: 616 Reward: -6.969539670435642 Epsilon: 7.378984651711924\n",
            "Total Timesteps: 6787 Episode Num: 617 Reward: -5.110063102752147 Epsilon: 7.367025195226168\n",
            "Total Timesteps: 6798 Episode Num: 618 Reward: -6.9423666360023795 Epsilon: 7.355104442483083\n",
            "Total Timesteps: 6809 Episode Num: 619 Reward: -4.838297980558335 Epsilon: 7.343222205903951\n",
            "Total Timesteps: 6820 Episode Num: 620 Reward: -5.183894082386634 Epsilon: 7.331378299120234\n",
            "Total Timesteps: 6831 Episode Num: 621 Reward: -5.274724375508808 Epsilon: 7.319572536963841\n",
            "Total Timesteps: 6842 Episode Num: 622 Reward: -7.215414595382896 Epsilon: 7.307804735457468\n",
            "Total Timesteps: 6853 Episode Num: 623 Reward: -6.000172628263468 Epsilon: 7.296074711805049\n",
            "Total Timesteps: 6864 Episode Num: 624 Reward: -6.011614418690364 Epsilon: 7.284382284382285\n",
            "Total Timesteps: 6875 Episode Num: 625 Reward: -5.221321750434467 Epsilon: 7.2727272727272725\n",
            "Total Timesteps: 6886 Episode Num: 626 Reward: -5.19209711075678 Epsilon: 7.261109497531223\n",
            "Total Timesteps: 6897 Episode Num: 627 Reward: -5.481227423696694 Epsilon: 7.249528780629259\n",
            "Total Timesteps: 6908 Episode Num: 628 Reward: -3.7042520573254256 Epsilon: 7.2379849449913145\n",
            "Total Timesteps: 6919 Episode Num: 629 Reward: -4.843391121598374 Epsilon: 7.226477814713109\n",
            "Total Timesteps: 6930 Episode Num: 630 Reward: -7.554688601754266 Epsilon: 7.215007215007215\n",
            "Total Timesteps: 6941 Episode Num: 631 Reward: -6.384251810538719 Epsilon: 7.203572972194208\n",
            "Total Timesteps: 6952 Episode Num: 632 Reward: -5.452839917362502 Epsilon: 7.192174913693901\n",
            "Total Timesteps: 6963 Episode Num: 633 Reward: -5.261902742327948 Epsilon: 7.18081286801666\n",
            "Total Timesteps: 6974 Episode Num: 634 Reward: -7.243895038902269 Epsilon: 7.169486664754803\n",
            "Total Timesteps: 6985 Episode Num: 635 Reward: -5.745093521799946 Epsilon: 7.158196134574087\n",
            "Total Timesteps: 6996 Episode Num: 636 Reward: -4.567004027721095 Epsilon: 7.14694110920526\n",
            "Total Timesteps: 7007 Episode Num: 637 Reward: -4.786401274355696 Epsilon: 7.135721421435707\n",
            "Total Timesteps: 7018 Episode Num: 638 Reward: -5.886924372146955 Epsilon: 7.124536905101168\n",
            "Total Timesteps: 7029 Episode Num: 639 Reward: -5.909127340266462 Epsilon: 7.1133873950775355\n",
            "Total Timesteps: 7040 Episode Num: 640 Reward: -5.696964788046477 Epsilon: 7.1022727272727275\n",
            "Total Timesteps: 7051 Episode Num: 641 Reward: -4.993870584491648 Epsilon: 7.091192738618636\n",
            "Total Timesteps: 7062 Episode Num: 642 Reward: -5.102544413854491 Epsilon: 7.080147267063155\n",
            "Total Timesteps: 7073 Episode Num: 643 Reward: -5.241014559674527 Epsilon: 7.0691361515622795\n",
            "Total Timesteps: 7084 Episode Num: 644 Reward: -5.205075467577266 Epsilon: 7.058159232072276\n",
            "Total Timesteps: 7095 Episode Num: 645 Reward: -4.890778339275734 Epsilon: 7.047216349541931\n",
            "Total Timesteps: 7106 Episode Num: 646 Reward: -6.3498659864508715 Epsilon: 7.036307345904869\n",
            "Total Timesteps: 7117 Episode Num: 647 Reward: -5.667614356985672 Epsilon: 7.02543206407194\n",
            "Total Timesteps: 7128 Episode Num: 648 Reward: -5.071959878611028 Epsilon: 7.014590347923681\n",
            "Total Timesteps: 7139 Episode Num: 649 Reward: -5.590292201330106 Epsilon: 7.003782042302843\n",
            "Total Timesteps: 7150 Episode Num: 650 Reward: -3.7793284327401784 Epsilon: 6.993006993006993\n",
            "Total Timesteps: 7161 Episode Num: 651 Reward: -6.632374461969912 Epsilon: 6.982265046781176\n",
            "Total Timesteps: 7172 Episode Num: 652 Reward: -5.97946626876393 Epsilon: 6.971556051310652\n",
            "Total Timesteps: 7183 Episode Num: 653 Reward: -5.391081666380595 Epsilon: 6.960879855213699\n",
            "Total Timesteps: 7194 Episode Num: 654 Reward: -6.15651480712516 Epsilon: 6.950236308034473\n",
            "Total Timesteps: 7205 Episode Num: 655 Reward: -5.3686472309915425 Epsilon: 6.939625260235947\n",
            "Total Timesteps: 7216 Episode Num: 656 Reward: -5.542560227840228 Epsilon: 6.929046563192904\n",
            "Total Timesteps: 7227 Episode Num: 657 Reward: -4.5343606896370545 Epsilon: 6.918500069185001\n",
            "Total Timesteps: 7238 Episode Num: 658 Reward: -7.1322675235064645 Epsilon: 6.907985631389887\n",
            "Total Timesteps: 7249 Episode Num: 659 Reward: -4.202966609707043 Epsilon: 6.897503103876397\n",
            "Total Timesteps: 7260 Episode Num: 660 Reward: -7.378853120474851 Epsilon: 6.887052341597796\n",
            "Total Timesteps: 7271 Episode Num: 661 Reward: -5.157199713578438 Epsilon: 6.876633200385092\n",
            "Total Timesteps: 7282 Episode Num: 662 Reward: -4.84502848005356 Epsilon: 6.866245536940401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcB82cJup6kp"
      },
      "source": [
        "## The inference policy function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUIPg-FCp-fG"
      },
      "source": [
        "def evaluate_final_policy(policy,random = 1, eval_episodes=1):\n",
        "  avg_reward = 0.\n",
        "  distance = 0.\n",
        "  for k in range(eval_episodes):\n",
        "    obs = env.reset(seed = random)\n",
        "    done = False\n",
        "    while not done:\n",
        "      action, current_Q = policy.select_action(obs, state_dim)\n",
        "      obs, reward, done = env.step(action)\n",
        "      avg_reward += reward\n",
        "    total_distance = 0\n",
        "    for j in range(len(env.routes)):\n",
        "      for i in range(len(env.routes[j])-1):\n",
        "        total_distance = total_distance + math.floor(((env.VRP[env.routes[j][i],0]-env.VRP[env.routes[j][i+1],0])**2+(env.VRP[env.routes[j][i],1]-env.VRP[env.routes[j][i+1],1])**2)**0.5)\n",
        "    distance += total_distance\n",
        "  distance /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Distance over the Evaluation Step: %f\" % (distance))\n",
        "  print (\"---------------------------------------\")\n",
        "  return distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZbeSFVKo1rd"
      },
      "source": [
        "## Let's see if we can test the implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Edox0ADo64E"
      },
      "source": [
        "results = np.zeros(1)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "for i in range(1):\n",
        "  evaluations = [evaluate_final_policy(policy, random = i)]\n",
        "  results[i] = evaluations[0]\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}