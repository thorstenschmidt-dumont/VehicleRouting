{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXu1r8qvSzWf"
   },
   "source": [
    "# Actor-critic learning for the CVRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xjm2onHdT-Av"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ikr2p0Js8iB4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import wrappers\n",
    "from torch.autograd import Variable\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2mkXSZ-JWOr"
   },
   "source": [
    "## Creating a CVRP environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5A98HsVVJbT5"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "\n",
    "class VRPEnv(gym.Env):\n",
    "  def __init__(self):\n",
    "    # customer count ('0' is depot) \n",
    "    self.customer_count = 11\n",
    "    # the capacity of vehicles\n",
    "    self.vehicle_capacity = 2\n",
    "  \n",
    "    self.action_space = spaces.Discrete(3)\n",
    "    self.observation_space = spaces.Box(low=0,high=1, shape=(4,1), dtype=np.float64)\n",
    "    self.VRP = np.array((self.customer_count,4))\n",
    "    self._max_episode_steps = 1000\n",
    "    self.viewer = None\n",
    "    self.state = None\n",
    "    self.steps_beyond_done = None\n",
    "    self.route = []\n",
    "    self.route.append(0)\n",
    "    self.previous_action = 0\n",
    "    self.hn_actor = torch.zeros([1,self.customer_count,128], dtype=torch.float32)\n",
    "    \n",
    "\n",
    "  def reset(self, seed=200):\n",
    "    if seed == 200:\n",
    "      seed = int(time.time())\n",
    "    np.random.seed(seed)\n",
    "    x_locations = (np.random.rand(self.customer_count)).reshape((self.customer_count,1))\n",
    "    y_locations = (np.random.rand(self.customer_count)).reshape((self.customer_count,1))\n",
    "    demand = (np.random.randint(1,9,self.customer_count).reshape((self.customer_count,1))).reshape((self.customer_count,1))/10 # Normalise to between 0.1 and 0.9\n",
    "    capacity = np.repeat(self.vehicle_capacity,self.customer_count).reshape((self.customer_count,1))\n",
    "    VRP = np.concatenate((np.concatenate((np.concatenate((x_locations,y_locations), axis=1),demand),axis=1),capacity),axis=1)\n",
    "    self.VRP = VRP.reshape((self.customer_count,4))\n",
    "    self.unserved_customers = []\n",
    "    for i in range(1, self.customer_count):\n",
    "      self.unserved_customers.append(i)\n",
    "    self.routes = []\n",
    "    self.route = []\n",
    "    self.route.append(0)\n",
    "    self.VRP[0,2] = 0 # Set the demand at thedepot to 0\n",
    "    self.state = copy.deepcopy(self.VRP)\n",
    "    self.previous_action = 0\n",
    "    return self.state\n",
    "  \n",
    "\n",
    "  def step(self, action):\n",
    "    # Calculate the reward as the negative euclidean distance\n",
    "    reward = -((self.state[self.previous_action,0]-self.state[action,0])**2+(self.state[self.previous_action,1]-self.state[action,1])**2)**0.5 # - Euclidean distance between customers\n",
    "    load = self.state[0,3]\n",
    "    self.state[:,3] = max(0,(load-self.state[action,2])) # Update the vehicle load\n",
    "    self.state[action, 2] = max(0,self.state[action,2]-load) # Update the demand at served customer\n",
    "    done = False\n",
    "    if action == 0: # Return to the depot\n",
    "      self.route.append(action) # End route\n",
    "      self.routes.append(self.route) # Add subroute to list of all routes\n",
    "      self.route = [] # Initiate new subroute\n",
    "      self.state[:,3] = self.vehicle_capacity # Refill the vehicle\n",
    "    self.route.append(action) # Add action to the subroute\n",
    "    if max(self.state[:,2]) > 0: # If there are unserved customers left\n",
    "      done = False\n",
    "    elif max(self.state[:,2]) == 0 and action == 0: # If there are no unserved customers left and we have returned to the depot\n",
    "      done = True\n",
    "      self.route.append(0)\n",
    "    self.previous_action = action # Update the previous action\n",
    "    return self.state, reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyy2xkZRNCRw"
   },
   "source": [
    "## Let's test the environment step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1626520460336,
     "user": {
      "displayName": "Thorsten Schmidt-Dumont",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh7YAfTkfw66husCiIJ1rhji7I5Ww8GpsHkztVw0Q=s64",
      "userId": "09121515534198097194"
     },
     "user_tz": -120
    },
    "id": "3_xu5wGbNINA",
    "outputId": "98de899c-a386-4aad-d400-6e4852673eef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69039862 0.06613502 0.         2.        ]\n",
      " [0.62200451 0.78504387 0.7        2.        ]\n",
      " [0.26096926 0.09412515 0.7        2.        ]\n",
      " [0.43824386 0.48512059 0.6        2.        ]\n",
      " [0.50067945 0.88418096 0.1        2.        ]\n",
      " [0.92344961 0.51373972 0.2        2.        ]\n",
      " [0.18505191 0.87495518 0.5        2.        ]\n",
      " [0.09287534 0.152164   0.2        2.        ]\n",
      " [0.35425302 0.32937999 0.7        2.        ]\n",
      " [0.7271695  0.295905   0.7        2.        ]\n",
      " [0.62360697 0.36830169 0.6        2.        ]]\n",
      "[[0.69039862 0.06613502 0.         1.3       ]\n",
      " [0.62200451 0.78504387 0.7        1.3       ]\n",
      " [0.26096926 0.09412515 0.         1.3       ]\n",
      " [0.43824386 0.48512059 0.6        1.3       ]\n",
      " [0.50067945 0.88418096 0.1        1.3       ]\n",
      " [0.92344961 0.51373972 0.2        1.3       ]\n",
      " [0.18505191 0.87495518 0.5        1.3       ]\n",
      " [0.09287534 0.152164   0.2        1.3       ]\n",
      " [0.35425302 0.32937999 0.7        1.3       ]\n",
      " [0.7271695  0.295905   0.7        1.3       ]\n",
      " [0.62360697 0.36830169 0.6        1.3       ]]\n"
     ]
    }
   ],
   "source": [
    "env = VRPEnv() # Create an instance of the environment\n",
    "state = env.reset() # Reset the environment\n",
    "action = 2 # Perform action with customer 2\n",
    "print(state)\n",
    "state, reward, done = env.step(action) # Perform the actual transition\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2nGdtlKVydr"
   },
   "source": [
    "## Initialize the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "u5rW0IDB8nTO"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, max_size=1e6):\n",
    "    self.storage = []\n",
    "    self.max_size = max_size\n",
    "    self.ptr = 0\n",
    "\n",
    "  def add(self, transition):\n",
    "    if len(self.storage) == self.max_size:\n",
    "      #self.storage[int(self.ptr)] = transition\n",
    "      #self.ptr = (self.ptr + 1) % self.max_size\n",
    "      self.storage.pop(0)\n",
    "      self.storage.append(transition)\n",
    "    else:\n",
    "      self.storage.append(transition)\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    ind =  np.arange((len(self.storage)-(batch_size+1)),len(self.storage)-1,1) #np.random.randint(0, len(self.storage), size=batch_size)\n",
    "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "    for i in ind: \n",
    "      state, next_state, action, reward, done = self.storage[i]\n",
    "      batch_states.append(np.array(state, copy=False))\n",
    "      batch_next_states.append(np.array(next_state, copy=False))\n",
    "      batch_actions.append(np.array(action, copy=False))\n",
    "      batch_rewards.append(np.array(reward, copy=False))\n",
    "      batch_dones.append(np.array(done, copy=False))\n",
    "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb7TTaHxWbQD"
   },
   "source": [
    "## Build the neural network for the Actor and Actor-target models - contains the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4CeRW4D79HL0"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim=4, embed_size = 128):#, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.embed = nn.Linear((state_dim), embed_size) # Encoding to higher dimensional space - can also be changed to convolutional layer as in the paper\n",
    "    self.u_t = nn.RNN(embed_size,embed_size,1) # RNN Layer for the attention mechanism\n",
    "    self.v_t_a = nn.Linear(embed_size,1) # Linear for getting u_t\n",
    "    self.bar_u_t = nn.RNN(embed_size,embed_size,1) # RNN Layer for the context vector\n",
    "    self.a_t = nn.Softmax(dim = 1) # Softmax layer for the attention mechanism\n",
    "    self.v_t_u = nn.Linear(embed_size,1) # Linear for getting u_t\n",
    "    self.final = nn.Softmax(dim = 1) # Softmax layer for the final output\n",
    "\n",
    "  def forward(self, x, hn = env.hn_actor):\n",
    "    cond1 = (x[:,:,2]<x[:,:,3]).int() # Can we meet the demand\n",
    "    cond2 = (x[:,:,2]>0).int() # Is there demand at the customer\n",
    "    mask1 = torch.minimum(cond1,cond2) # Select those customers with demand, and whose demand we can meet\n",
    "    mask1 = torch.reshape(mask1,(len(x),env.customer_count,1))\n",
    "    x = self.embed(x)\n",
    "    u, hn = self.u_t(x, hn)\n",
    "    u = self.v_t_a(u)\n",
    "    a = self.a_t(u) # Up to equation (4) now\n",
    "    c = torch.randn(x.shape)\n",
    "    c = torch.mul(x,a)\n",
    "    c = torch.sum(c, 0)\n",
    "    c = torch.reshape(c,(1,env.customer_count,128))\n",
    "    u_bar, hu = self.bar_u_t(x,c)\n",
    "    u_bar = self.v_t_u(u_bar)\n",
    "    output = self.final(u_bar)\n",
    "    output = torch.mul(output,mask1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDRPWgEVNRcF"
   },
   "source": [
    "## Build the neural network for the Critic and Critic-target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_z9ifdVdNRLO"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim=4, action_dim = env.customer_count, embed_size = 128):\n",
    "    super(Critic, self).__init__()\n",
    "    # Defining the first Critic neural network\n",
    "    self.layer_1 = nn.Linear(state_dim, embed_size) # Perform the embedding\n",
    "    self.layer_2 = nn.Linear(embed_size, embed_size) # Adding the single dense layer\n",
    "    self.layer_3 = nn.Linear(embed_size, 1) # Adding the output layer\n",
    "\n",
    "\n",
    "  def forward(self, x, u): # x is the state, u is the action\n",
    "    # Forward-Propagation on the Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(x))\n",
    "    ws = torch.mul(x1,u)\n",
    "    x2 = F.relu(self.layer_2(ws))\n",
    "    x2 = self.layer_3(x2)\n",
    "    x2 = torch.sum(x2,1)\n",
    "    return x2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VmkiLbHonzy"
   },
   "source": [
    "## Testing the actor and critic networks to confirm their output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1626520460887,
     "user": {
      "displayName": "Thorsten Schmidt-Dumont",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh7YAfTkfw66husCiIJ1rhji7I5Ww8GpsHkztVw0Q=s64",
      "userId": "09121515534198097194"
     },
     "user_tz": -120
    },
    "id": "sQ_-V52dw76O",
    "outputId": "477249c6-6cef-43fb-df65-c83475e67ea8"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'minimum'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-241f5d6cedf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mactor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mActor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mactor_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mActor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprediction_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-99038a5cfb76>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, hn)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mcond1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Can we meet the demand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mcond2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Is there demand at the customer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mmask1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcond2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Select those customers with demand, and whose demand we can meet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mmask1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcustomer_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'minimum'"
     ]
    }
   ],
   "source": [
    "env = VRPEnv()\n",
    "env.reset()\n",
    "state = env.reset()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state = torch.Tensor(state.reshape(1,env.customer_count,4)).to(device)\n",
    "actor = Actor()\n",
    "actor_target = Actor()\n",
    "prediction = actor(state)\n",
    "prediction_target = actor_target(state)\n",
    "print(prediction)\n",
    "critic = Critic()\n",
    "q_value = critic(state,prediction)\n",
    "print(q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzIDuONodenW"
   },
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzd0H1xukdKe"
   },
   "outputs": [],
   "source": [
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Building the Training Process into a class\n",
    "class Actor_Critic(object):\n",
    "  \n",
    "  def __init__(self, state_dim):\n",
    "    self.actor = Actor(state_dim).to(device)\n",
    "    self.actor_target = Actor(state_dim).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = 0.0001)\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state, state_dim):\n",
    "    state_tensor = torch.Tensor(state.reshape(1,env.customer_count,state_dim)).to(device)\n",
    "    current_Q = actor(state_tensor).detach()\n",
    "    current_Q = current_Q.detach().numpy().reshape(env.customer_count)\n",
    "    action = np.argmax(current_Q)\n",
    "    return action, current_Q\n",
    "\n",
    "  def select_target_action(self, state):\n",
    "    state_tensor = torch.Tensor(state.reshape(1,env.customer_count,state_dim)).to(device)\n",
    "    target_Q = actor_target(state_tensor).detach()\n",
    "    target_Q = target_Q.detach().numpy().reshape(env.customer_count)\n",
    "    action = torch.argmax(target_Q)\n",
    "    return action, target_Q\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=32, discount=0.99, tau=0.005):\n",
    "    \n",
    "    for it in range(iterations):\n",
    "      \n",
    "      # Sample a batch of transitions (s, sâ€™, a, r) from the memory\n",
    "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "      state = torch.Tensor(batch_states).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "\n",
    "      # Determine the next action based on the actor target\n",
    "      next_action = self.actor_target(next_state)\n",
    "      \n",
    "      # Determine the target Q-value based on the critic target\n",
    "      target_Q = self.critic_target(next_state, next_action)\n",
    "\n",
    "      # Get the final target using the RL update rule\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "\n",
    "      # Determine the probabilities of the actions chosen by the actor\n",
    "      action = self.actor(state)\n",
    "\n",
    "      # Determine the current Q-value based on the critic\n",
    "      current_Q = self.critic(state,action)\n",
    "\n",
    "      # Compute the loss coming from the critic models: Critic Loss = MSE_Loss(Q, Qt)\n",
    "      critic_loss = F.mse_loss(current_Q, target_Q)\n",
    "\n",
    "      # Backpropagate this Critic loss and update the parameters of the Critic model with Adam optimizer\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward(retain_graph=True)\n",
    "      self.critic_optimizer.step()\n",
    "\n",
    "      # Update our Actor model by performing gradient descent on the output of the Critic model  \n",
    "      actor_loss = -self.critic(state, action).mean()\n",
    "      self.actor_optimizer.zero_grad()\n",
    "      actor_loss.backward()\n",
    "      self.actor_optimizer.step()\n",
    "\n",
    "      # Update the weights of the Actor target by polyak averaging\n",
    "      for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "      # Update the weights of the Critic target by polyak averaging\n",
    "      for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "  \n",
    "  # Make a save method to save a trained model\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ka-ZRtQvjBex"
   },
   "source": [
    "## Create a function that evaluates the policy by calculating its average reward over 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qabqiYdp9wDM"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action, current_Q = policy.select_action(obs, state_dim)\n",
    "      obs, reward, done = env.step(action)\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGuKmH_ijf7U"
   },
   "source": [
    "## Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFj6wbAo97lk"
   },
   "outputs": [],
   "source": [
    "env_name = \"CVRP\" # Name of a environment (set it to any Continous environment you want)\n",
    "seed = 0 # Random seed number\n",
    "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
    "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
    "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
    "batch_size = 128 # Size of the batch\n",
    "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.001 # Target network update rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hjwf2HCol3XP"
   },
   "source": [
    "## Create a file name for the two saved models: the Actor and Critic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1626520460888,
     "user": {
      "displayName": "Thorsten Schmidt-Dumont",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh7YAfTkfw66husCiIJ1rhji7I5Ww8GpsHkztVw0Q=s64",
      "userId": "09121515534198097194"
     },
     "user_tz": -120
    },
    "id": "1fyH8N5z-o3o",
    "outputId": "6af9d8eb-1c4e-47fb-fe62-7fe57ef613fc"
   },
   "outputs": [],
   "source": [
    "file_name = \"%s_%s_%s\" % (\"Actor_Critic\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kop-C96Aml8O"
   },
   "source": [
    "## Create a folder to save the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Src07lvY-zXb"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./results\"):\n",
    "  os.makedirs(\"./results\")\n",
    "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
    "  os.makedirs(\"./pytorch_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEAzOd47mv1Z"
   },
   "source": [
    "## Create an instance of the CVRP environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyQXJUIs-6BV"
   },
   "outputs": [],
   "source": [
    "env = VRPEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YdPG4HXnNsh"
   },
   "source": [
    "## Set seeds and get the necessary information on the states and actions in the chosen environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3RufYec_ADj"
   },
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.customer_count\n",
    "max_action = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWEgDAQxnbem"
   },
   "source": [
    "## Create the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1626520460889,
     "user": {
      "displayName": "Thorsten Schmidt-Dumont",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh7YAfTkfw66husCiIJ1rhji7I5Ww8GpsHkztVw0Q=s64",
      "userId": "09121515534198097194"
     },
     "user_tz": -120
    },
    "id": "wTVvG7F8_EWg",
    "outputId": "6826b2ca-b0bd-4aa3-9d79-8bf874008264"
   },
   "outputs": [],
   "source": [
    "print(state_dim, action_dim, max_action)\n",
    "policy = Actor_Critic(state_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZI60VN2Unklh"
   },
   "source": [
    "## Create the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sd-ZsdXR_LgV"
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYOpCyiDnw7s"
   },
   "source": [
    "## Define a list where all the evaluation results over 10 episodes are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1626520461299,
     "user": {
      "displayName": "Thorsten Schmidt-Dumont",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh7YAfTkfw66husCiIJ1rhji7I5Ww8GpsHkztVw0Q=s64",
      "userId": "09121515534198097194"
     },
     "user_tz": -120
    },
    "id": "dhC_5XJ__Orp",
    "outputId": "a7c174e5-5d92-4f36-f214-dd27e94fa77e"
   },
   "outputs": [],
   "source": [
    "evaluations = [evaluate_policy(policy, eval_episodes=1)]\n",
    "print(env.routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1626520461300,
     "user": {
      "displayName": "Thorsten Schmidt-Dumont",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh7YAfTkfw66husCiIJ1rhji7I5Ww8GpsHkztVw0Q=s64",
      "userId": "09121515534198097194"
     },
     "user_tz": -120
    },
    "id": "6t3LV3qZaWkt",
    "outputId": "33be08e6-9448-4858-eb0a-92942fd7ac43"
   },
   "outputs": [],
   "source": [
    "print(env.routes)\n",
    "print(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xm-4b3p6rglE"
   },
   "source": [
    "## Create a folder directory in which the final results will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTL9uMd0ru03"
   },
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "max_episode_steps = env._max_episode_steps\n",
    "save_env_vid = False\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31n5eb03p-Fm"
   },
   "source": [
    "## Initialize the training process variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vN5EvxK_QhT"
   },
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9gsjvtPqLgT"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_ouY4NH_Y0I",
    "outputId": "199db34d-5086-4766-8137-222909fcf531"
   },
   "outputs": [],
   "source": [
    "# We start the main loop over 500,000 timesteps\n",
    "env.reset()\n",
    "total_timesteps = 0\n",
    "obs = copy.deepcopy(env.reset())\n",
    "maximum = -1000\n",
    "while total_timesteps < max_timesteps:\n",
    "  \n",
    "  # If the episode is done\n",
    "  if done:\n",
    "    \n",
    "    # If we are not at the very beginning, we start the training process of the model\n",
    "    if total_timesteps != 0 and total_timesteps > batch_size:\n",
    "      print(\"Total Timesteps: {} Episode Num: {} Reward: {} Epsilon: {}\".format(total_timesteps, episode_num, episode_reward, epsilon))\n",
    "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau)\n",
    "\n",
    "    # We evaluate the episode and we save the policy\n",
    "    if timesteps_since_eval >= eval_freq:\n",
    "      timesteps_since_eval %= eval_freq\n",
    "      print(\"Total timesteps: {} Epsilon: {}\".format(total_timesteps, min(1,epsilon)))\n",
    "      evaluations.append(evaluate_policy(policy))\n",
    "      if evaluations[len(evaluations)-1] > maximum:\n",
    "        policy.save(file_name, directory=\"./pytorch_models\")\n",
    "        maximum = evaluations[len(evaluations)-1]\n",
    "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "    \n",
    "    # When the training step is done, we reset the state of the environment\n",
    "    obs = copy.deepcopy(env.reset())\n",
    "    \n",
    "    # Set the Done to False\n",
    "    done = False\n",
    "    \n",
    "    # Set rewards and episode timesteps to zero\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1\n",
    "  \n",
    "  # Work with epsilon-greedy\n",
    "  epsilon = 50000/(total_timesteps+1)\n",
    "  np.random.seed(total_timesteps)\n",
    "  if np.random.rand() < min(1,epsilon):\n",
    "    feasible = np.array([0])\n",
    "    for i in range(1,env.customer_count):\n",
    "      if obs[i,2] < obs[i,3] and obs[i,2] != 0:\n",
    "        feasible = np.concatenate((feasible,np.array([i])))\n",
    "    if len(feasible) > 1:\n",
    "      feasible = np.delete(feasible,0)\n",
    "    action = np.random.choice(feasible)\n",
    "  else: # Choose greedy action\n",
    "    action, current_Q = policy.select_action(obs, state_dim)\n",
    "  \n",
    "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
    "  new_obs, reward, done = env.step(action)\n",
    "\n",
    "  # Check if the episode is done\n",
    "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
    "\n",
    "  # Increase the total reward\n",
    "  episode_reward += reward\n",
    "\n",
    "  # Store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "  replay_buffer.add((obs, copy.deepcopy(new_obs), action, reward, done_bool)) # Have to use deepcopy to prevent overwriting of historic next states\n",
    "\n",
    "  # Update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
    "  obs = copy.deepcopy(new_obs)\n",
    "  episode_timesteps += 1\n",
    "  total_timesteps += 1\n",
    "  timesteps_since_eval += 1\n",
    "\n",
    "# Add the last policy evaluation to our list of evaluations and we save our model\n",
    "evaluations.append(evaluate_policy(policy))\n",
    "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
    "np.save(\"./results/%s\" % (file_name), evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcB82cJup6kp"
   },
   "source": [
    "## The inference policy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUIPg-FCp-fG"
   },
   "outputs": [],
   "source": [
    "def evaluate_final_policy(policy,random = 1, eval_episodes=1):\n",
    "  avg_reward = 0.\n",
    "  distance = 0.\n",
    "  for k in range(eval_episodes):\n",
    "    obs = env.reset(seed = random)\n",
    "    done = False\n",
    "    while not done:\n",
    "      action, current_Q = policy.select_action(obs, state_dim)\n",
    "      obs, reward, done = env.step(action)\n",
    "      avg_reward += reward\n",
    "    total_distance = 0\n",
    "    for j in range(len(env.routes)):\n",
    "      for i in range(len(env.routes[j])-1):\n",
    "        total_distance = total_distance + math.floor(((env.VRP[env.routes[j][i],0]-env.VRP[env.routes[j][i+1],0])**2+(env.VRP[env.routes[j][i],1]-env.VRP[env.routes[j][i+1],1])**2)**0.5)\n",
    "    distance += total_distance\n",
    "  distance /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Distance over the Evaluation Step: %f\" % (distance))\n",
    "  print (\"---------------------------------------\")\n",
    "  return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZbeSFVKo1rd"
   },
   "source": [
    "## Let's see if we can test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Edox0ADo64E"
   },
   "outputs": [],
   "source": [
    "results = np.zeros(1)\n",
    "policy.load(file_name, './pytorch_models/')\n",
    "for i in range(1):\n",
    "  evaluations = [evaluate_final_policy(policy, random = i)]\n",
    "  results[i] = evaluations[0]\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CartrackRLAttempt_V3.ipynb",
   "provenance": [
    {
     "file_id": "18mGk561T8mmc0CiCujUtmw7OmHuHYbVq",
     "timestamp": 1626166823032
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
